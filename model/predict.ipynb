{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path=\"/home/u131168/mh_one_api/data/train.csv\"\n",
    "ftrain_path=\"/home/u131168/mh_one_api/data/train_split/full_traind.csv\"\n",
    "\n",
    "test_path=\"/home/u131168/mh_one_api/data/test.csv\"\n",
    "submission_path=\"/home/u131168/mh_one_api/data/submission.csv\"\n",
    "\n",
    "train_data=pd.read_csv(train_path)\n",
    "ftrain_data=pd.read_csv(ftrain_path)\n",
    "\n",
    "test_data=pd.read_csv(test_path)\n",
    "submission_data=pd.read_csv(submission_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# story=ftrain_data.iloc[0,0]\n",
    "# question=ftrain_data.iloc[0,1]\n",
    "# ftrain_data.iloc[:15]\n",
    "\n",
    "# ftrain_data.iloc[:,2].to_csv(\"/home/u131168/mh_one_api/data/train_split/train_output.csv\",)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in /opt/miniconda/lib/python3.10/site-packages (0.41.1)\n",
      "Requirement already satisfied: accelerate in /home/u131168/.local/lib/python3.10/site-packages (0.23.0)\n",
      "Requirement already satisfied: psutil in /home/u131168/.local/lib/python3.10/site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: huggingface-hub in /home/u131168/.local/lib/python3.10/site-packages (from accelerate) (0.16.4)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/u131168/.local/lib/python3.10/site-packages (from accelerate) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda/lib/python3.10/site-packages (from accelerate) (23.0)\n",
      "Requirement already satisfied: pyyaml in /home/u131168/.local/lib/python3.10/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/u131168/.local/lib/python3.10/site-packages (from accelerate) (1.26.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/u131168/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/u131168/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.2.10.91)\n",
      "Requirement already satisfied: filelock in /home/u131168/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.12.4)\n",
      "Requirement already satisfied: jinja2 in /home/u131168/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/u131168/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/u131168/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.14.3)\n",
      "Requirement already satisfied: typing-extensions in /home/u131168/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.8.0)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/u131168/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.7.91)\n",
      "Requirement already satisfied: sympy in /home/u131168/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/u131168/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/u131168/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/u131168/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/u131168/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.7.101)\n",
      "Requirement already satisfied: networkx in /home/u131168/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/u131168/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/u131168/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.7.99)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/u131168/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /opt/miniconda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate) (65.6.3)\n",
      "Requirement already satisfied: wheel in /opt/miniconda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate) (0.38.4)\n",
      "Requirement already satisfied: lit in /home/u131168/.local/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (16.0.6)\n",
      "Requirement already satisfied: cmake in /home/u131168/.local/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/miniconda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.65.0)\n",
      "Requirement already satisfied: requests in /opt/miniconda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.28.1)\n",
      "Requirement already satisfied: fsspec in /home/u131168/.local/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2023.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/u131168/.local/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/miniconda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/miniconda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/u131168/.local/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: scipy in /opt/miniconda/lib/python3.10/site-packages (1.11.3)\n",
      "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /home/u131168/.local/lib/python3.10/site-packages (from scipy) (1.26.0)\n",
      "Requirement already satisfied: transformers in /opt/miniconda/lib/python3.10/site-packages (4.34.0.dev0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/u131168/.local/lib/python3.10/site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/u131168/.local/lib/python3.10/site-packages (from transformers) (1.26.0)\n",
      "Requirement already satisfied: filelock in /home/u131168/.local/lib/python3.10/site-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/u131168/.local/lib/python3.10/site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/u131168/.local/lib/python3.10/site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: requests in /opt/miniconda/lib/python3.10/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /home/u131168/.local/lib/python3.10/site-packages (from transformers) (0.14.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/miniconda/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda/lib/python3.10/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/u131168/.local/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: fsspec in /home/u131168/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/u131168/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/miniconda/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/miniconda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: peft in /home/u131168/.local/lib/python3.10/site-packages (0.5.0)\n",
      "Requirement already satisfied: safetensors in /home/u131168/.local/lib/python3.10/site-packages (from peft) (0.3.3)\n",
      "Requirement already satisfied: psutil in /home/u131168/.local/lib/python3.10/site-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /home/u131168/.local/lib/python3.10/site-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: transformers in /opt/miniconda/lib/python3.10/site-packages (from peft) (4.34.0.dev0)\n",
      "Requirement already satisfied: accelerate in /home/u131168/.local/lib/python3.10/site-packages (from peft) (0.23.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /home/u131168/.local/lib/python3.10/site-packages (from peft) (2.0.1)\n",
      "Requirement already satisfied: tqdm in /opt/miniconda/lib/python3.10/site-packages (from peft) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/u131168/.local/lib/python3.10/site-packages (from peft) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda/lib/python3.10/site-packages (from peft) (23.0)\n",
      "Requirement already satisfied: filelock in /home/u131168/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.12.4)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/u131168/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (10.9.0.58)\n",
      "Requirement already satisfied: sympy in /home/u131168/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/u131168/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/u131168/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/u131168/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/u131168/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/u131168/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.7.99)\n",
      "Requirement already satisfied: jinja2 in /home/u131168/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/u131168/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (2.0.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/u131168/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/u131168/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.7.101)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/u131168/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.7.91)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/u131168/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.7.4.91)\n",
      "Requirement already satisfied: networkx in /home/u131168/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/u131168/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (2.14.3)\n",
      "Requirement already satisfied: typing-extensions in /home/u131168/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (4.8.0)\n",
      "Requirement already satisfied: setuptools in /opt/miniconda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.13.0->peft) (65.6.3)\n",
      "Requirement already satisfied: wheel in /opt/miniconda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.13.0->peft) (0.38.4)\n",
      "Requirement already satisfied: lit in /home/u131168/.local/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.13.0->peft) (16.0.6)\n",
      "Requirement already satisfied: cmake in /home/u131168/.local/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.13.0->peft) (3.27.5)\n",
      "Requirement already satisfied: huggingface-hub in /home/u131168/.local/lib/python3.10/site-packages (from accelerate->peft) (0.16.4)\n",
      "Requirement already satisfied: requests in /opt/miniconda/lib/python3.10/site-packages (from transformers->peft) (2.28.1)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /home/u131168/.local/lib/python3.10/site-packages (from transformers->peft) (0.14.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/u131168/.local/lib/python3.10/site-packages (from transformers->peft) (2023.8.8)\n",
      "Requirement already satisfied: fsspec in /home/u131168/.local/lib/python3.10/site-packages (from huggingface-hub->accelerate->peft) (2023.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/u131168/.local/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/miniconda/lib/python3.10/site-packages (from requests->transformers->peft) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/miniconda/lib/python3.10/site-packages (from requests->transformers->peft) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda/lib/python3.10/site-packages (from requests->transformers->peft) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda/lib/python3.10/site-packages (from requests->transformers->peft) (3.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/u131168/.local/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4f76c9e29fb402091e839c00ac0ccd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "HFValidationError",
     "evalue": "Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/u131168/mh_one_api/model/ft_models/flan-t5-xl_peft_ft_v1/checkpoint-32600'. Use `repo_type` argument if needed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/u131168/mh_one_api/model/predict.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bmh_one_api/home/u131168/mh_one_api/model/predict.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m model_path\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/home/u131168/mh_one_api/model/ft_models/flan-t5-xl_peft_ft_v1/checkpoint-32600\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bmh_one_api/home/u131168/mh_one_api/model/predict.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mgoogle/flan-t5-xl\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://tunnel%2Bmh_one_api/home/u131168/mh_one_api/model/predict.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m custom_model \u001b[39m=\u001b[39m AutoPeftModelForSeq2SeqLM\u001b[39m.\u001b[39;49mfrom_pretrained(model_path, )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/peft/auto.py:103\u001b[0m, in \u001b[0;36m_BaseAutoPeftModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, adapter_name, is_trainable, config, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     98\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot infer the auto class from the config, please make sure that you are loading the correct model for your task type.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m     )\n\u001b[1;32m    101\u001b[0m base_model \u001b[39m=\u001b[39m target_class\u001b[39m.\u001b[39mfrom_pretrained(base_model_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_target_peft_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    104\u001b[0m     base_model,\n\u001b[1;32m    105\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m    106\u001b[0m     adapter_name\u001b[39m=\u001b[39;49madapter_name,\n\u001b[1;32m    107\u001b[0m     is_trainable\u001b[39m=\u001b[39;49mis_trainable,\n\u001b[1;32m    108\u001b[0m     config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    109\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    110\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/peft/peft_model.py:278\u001b[0m, in \u001b[0;36mPeftModel.from_pretrained\u001b[0;34m(cls, model, model_id, adapter_name, is_trainable, config, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m     model \u001b[39m=\u001b[39m MODEL_TYPE_TO_PEFT_MODEL_MAPPING[config\u001b[39m.\u001b[39mtask_type](model, config, adapter_name)\n\u001b[0;32m--> 278\u001b[0m model\u001b[39m.\u001b[39;49mload_adapter(model_id, adapter_name, is_trainable\u001b[39m=\u001b[39;49mis_trainable, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    279\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/peft/peft_model.py:554\u001b[0m, in \u001b[0;36mPeftModel.load_adapter\u001b[0;34m(self, model_id, adapter_name, is_trainable, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m         peft_config\u001b[39m.\u001b[39minference_mode \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m is_trainable\n\u001b[1;32m    552\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_adapter(adapter_name, peft_config)\n\u001b[0;32m--> 554\u001b[0m adapters_weights \u001b[39m=\u001b[39m load_peft_weights(model_id, device\u001b[39m=\u001b[39;49mtorch_device, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhf_hub_download_kwargs)\n\u001b[1;32m    556\u001b[0m \u001b[39m# load the weights into the model\u001b[39;00m\n\u001b[1;32m    557\u001b[0m load_result \u001b[39m=\u001b[39m set_peft_model_state_dict(\u001b[39mself\u001b[39m, adapters_weights, adapter_name\u001b[39m=\u001b[39madapter_name)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:171\u001b[0m, in \u001b[0;36mload_peft_weights\u001b[0;34m(model_id, device, **hf_hub_download_kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m     use_safetensors \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 171\u001b[0m     has_remote_safetensors_file \u001b[39m=\u001b[39m hub_file_exists(\n\u001b[1;32m    172\u001b[0m         model_id,\n\u001b[1;32m    173\u001b[0m         SAFETENSORS_WEIGHTS_NAME,\n\u001b[1;32m    174\u001b[0m         revision\u001b[39m=\u001b[39;49mhf_hub_download_kwargs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mrevision\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    175\u001b[0m         repo_type\u001b[39m=\u001b[39;49mhf_hub_download_kwargs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mrepo_type\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    176\u001b[0m     )\n\u001b[1;32m    177\u001b[0m     use_safetensors \u001b[39m=\u001b[39m has_remote_safetensors_file\n\u001b[1;32m    179\u001b[0m     \u001b[39mif\u001b[39;00m has_remote_safetensors_file:\n\u001b[1;32m    180\u001b[0m         \u001b[39m# Priority 1: load safetensors weights\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/peft/utils/hub_utils.py:24\u001b[0m, in \u001b[0;36mhub_file_exists\u001b[0;34m(repo_id, filename, revision, repo_type)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhub_file_exists\u001b[39m(repo_id: \u001b[39mstr\u001b[39m, filename: \u001b[39mstr\u001b[39m, revision: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, repo_type: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[1;32m     21\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39m    Checks if a file exists in a remote Hub repository.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     url \u001b[39m=\u001b[39m hf_hub_url(repo_id\u001b[39m=\u001b[39;49mrepo_id, filename\u001b[39m=\u001b[39;49mfilename, repo_type\u001b[39m=\u001b[39;49mrepo_type, revision\u001b[39m=\u001b[39;49mrevision)\n\u001b[1;32m     25\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m         get_hf_file_metadata(url)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:110\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39mfor\u001b[39;00m arg_name, arg_value \u001b[39min\u001b[39;00m chain(\n\u001b[1;32m    106\u001b[0m     \u001b[39mzip\u001b[39m(signature\u001b[39m.\u001b[39mparameters, args),  \u001b[39m# Args values\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     kwargs\u001b[39m.\u001b[39mitems(),  \u001b[39m# Kwargs values\u001b[39;00m\n\u001b[1;32m    108\u001b[0m ):\n\u001b[1;32m    109\u001b[0m     \u001b[39mif\u001b[39;00m arg_name \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mrepo_id\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfrom_id\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mto_id\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m--> 110\u001b[0m         validate_repo_id(arg_value)\n\u001b[1;32m    112\u001b[0m     \u001b[39melif\u001b[39;00m arg_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtoken\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m arg_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m         has_token \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:158\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[39mraise\u001b[39;00m HFValidationError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRepo id must be a string, not \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(repo_id)\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mrepo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    157\u001b[0m \u001b[39mif\u001b[39;00m repo_id\u001b[39m.\u001b[39mcount(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[39mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    159\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRepo id must be in the form \u001b[39m\u001b[39m'\u001b[39m\u001b[39mrepo_name\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mnamespace/repo_name\u001b[39m\u001b[39m'\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mrepo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. Use `repo_type` argument if needed.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m REPO_ID_REGEX\u001b[39m.\u001b[39mmatch(repo_id):\n\u001b[1;32m    164\u001b[0m     \u001b[39mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    165\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRepo id must use alphanumeric chars or \u001b[39m\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m--\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39m..\u001b[39m\u001b[39m'\u001b[39m\u001b[39m are\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    166\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m forbidden, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m cannot start or end the name, max length is 96:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    167\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mrepo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m     )\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/u131168/mh_one_api/model/ft_models/flan-t5-xl_peft_ft_v1/checkpoint-32600'. Use `repo_type` argument if needed."
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install bitsandbytes\n",
    "!pip install accelerate\n",
    "!pip install scipy\n",
    "!pip install transformers\n",
    "!pip install peft\n",
    "from peft import AutoPeftModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "# model_path=\"/home/u131168/mh_one_api/model/ft_models/flan-t5-xl_peft_finetuned_model/checkpoint-36000\"\n",
    "# model_path=\"/home/u131168/mh_one_api/model/ft_models/flan-t5-xl_peft_ss/checkpoint-7000\"\n",
    "model_path=\"/home/u131168/mh_one_api/model/ft_models/flan-t5-xl_peft_ft_v1/checkpoint-32900\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xl\")\n",
    "custom_model = AutoPeftModelForSeq2SeqLM.from_pretrained(model_path, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/u131168/mh_one_api/model/predict.ipynb Cell 5\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bmh_one_api/home/u131168/mh_one_api/model/predict.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m model_path\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgoogle/flan-t5-xl\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bmh_one_api/home/u131168/mh_one_api/model/predict.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_path)\n\u001b[0;32m----> <a href='vscode-notebook-cell://tunnel%2Bmh_one_api/home/u131168/mh_one_api/model/predict.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m ft5_model \u001b[39m=\u001b[39m AutoModelForSeq2SeqLM\u001b[39m.\u001b[39;49mfrom_pretrained(model_path)\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    562\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 563\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    564\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    565\u001b[0m     )\n\u001b[1;32m    566\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m )\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.10/site-packages/transformers/modeling_utils.py:3067\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3064\u001b[0m     config \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_check_and_enable_flash_attn_2(config, torch_dtype\u001b[39m=\u001b[39mtorch_dtype, device_map\u001b[39m=\u001b[39mdevice_map)\n\u001b[1;32m   3066\u001b[0m \u001b[39mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[0;32m-> 3067\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(config, \u001b[39m*\u001b[39;49mmodel_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[1;32m   3069\u001b[0m \u001b[39m# Check first if we are `from_pt`\u001b[39;00m\n\u001b[1;32m   3070\u001b[0m \u001b[39mif\u001b[39;00m use_keep_in_fp32_modules:\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1579\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m   1577\u001b[0m decoder_config\u001b[39m.\u001b[39mis_encoder_decoder \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1578\u001b[0m decoder_config\u001b[39m.\u001b[39mnum_layers \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mnum_decoder_layers\n\u001b[0;32m-> 1579\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder \u001b[39m=\u001b[39m T5Stack(decoder_config, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshared)\n\u001b[1;32m   1581\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(config\u001b[39m.\u001b[39md_model, config\u001b[39m.\u001b[39mvocab_size, bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m   1583\u001b[0m \u001b[39m# Initialize weights and apply final processing\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:916\u001b[0m, in \u001b[0;36mT5Stack.__init__\u001b[0;34m(self, config, embed_tokens)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_tokens \u001b[39m=\u001b[39m embed_tokens\n\u001b[1;32m    913\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_decoder \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mis_decoder\n\u001b[1;32m    915\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblock \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList(\n\u001b[0;32m--> 916\u001b[0m     [T5Block(config, has_relative_attention_bias\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m(i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m)) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mnum_layers)]\n\u001b[1;32m    917\u001b[0m )\n\u001b[1;32m    918\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinal_layer_norm \u001b[39m=\u001b[39m T5LayerNorm(config\u001b[39m.\u001b[39md_model, eps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mlayer_norm_epsilon)\n\u001b[1;32m    919\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout(config\u001b[39m.\u001b[39mdropout_rate)\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:916\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_tokens \u001b[39m=\u001b[39m embed_tokens\n\u001b[1;32m    913\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_decoder \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mis_decoder\n\u001b[1;32m    915\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblock \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList(\n\u001b[0;32m--> 916\u001b[0m     [T5Block(config, has_relative_attention_bias\u001b[39m=\u001b[39;49m\u001b[39mbool\u001b[39;49m(i \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m)) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mnum_layers)]\n\u001b[1;32m    917\u001b[0m )\n\u001b[1;32m    918\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinal_layer_norm \u001b[39m=\u001b[39m T5LayerNorm(config\u001b[39m.\u001b[39md_model, eps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mlayer_norm_epsilon)\n\u001b[1;32m    919\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout(config\u001b[39m.\u001b[39mdropout_rate)\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:659\u001b[0m, in \u001b[0;36mT5Block.__init__\u001b[0;34m(self, config, has_relative_attention_bias)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer\u001b[39m.\u001b[39mappend(T5LayerSelfAttention(config, has_relative_attention_bias\u001b[39m=\u001b[39mhas_relative_attention_bias))\n\u001b[1;32m    658\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_decoder:\n\u001b[0;32m--> 659\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer\u001b[39m.\u001b[39mappend(T5LayerCrossAttention(config))\n\u001b[1;32m    661\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer\u001b[39m.\u001b[39mappend(T5LayerFF(config))\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:619\u001b[0m, in \u001b[0;36mT5LayerCrossAttention.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, config):\n\u001b[1;32m    618\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m--> 619\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mEncDecAttention \u001b[39m=\u001b[39m T5Attention(config, has_relative_attention_bias\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    620\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm \u001b[39m=\u001b[39m T5LayerNorm(config\u001b[39m.\u001b[39md_model, eps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mlayer_norm_epsilon)\n\u001b[1;32m    621\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout(config\u001b[39m.\u001b[39mdropout_rate)\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:365\u001b[0m, in \u001b[0;36mT5Attention.__init__\u001b[0;34m(self, config, has_relative_attention_bias)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_model, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minner_dim, bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    364\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_model, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minner_dim, bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 365\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mLinear(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49md_model, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minner_dim, bias\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    366\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mo \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minner_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_model, bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    368\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_relative_attention_bias:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:101\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregister_parameter(\u001b[39m'\u001b[39m\u001b[39mbias\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreset_parameters()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:107\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreset_parameters\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[39m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[39m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[39m# https://github.com/pytorch/pytorch/issues/57109\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     init\u001b[39m.\u001b[39;49mkaiming_uniform_(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, a\u001b[39m=\u001b[39;49mmath\u001b[39m.\u001b[39;49msqrt(\u001b[39m5\u001b[39;49m))\n\u001b[1;32m    108\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m         fan_in, _ \u001b[39m=\u001b[39m init\u001b[39m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/init.py:412\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity)\u001b[0m\n\u001b[1;32m    410\u001b[0m bound \u001b[39m=\u001b[39m math\u001b[39m.\u001b[39msqrt(\u001b[39m3.0\u001b[39m) \u001b[39m*\u001b[39m std  \u001b[39m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 412\u001b[0m     \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49muniform_(\u001b[39m-\u001b[39;49mbound, bound)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # Load model directly\n",
    "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "# model_path=\"google/flan-t5-xl\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# ft5_model = AutoModelForSeq2SeqLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# prompt = \"Today I believe we can finally\"\n",
    "# prompt = [[\"Please answer to the following question.\",\" Who is going to be the next john wick d'or?\"], ]\n",
    "# # print(question,story)\n",
    "# prompt=[[story,question]]\n",
    "# input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "# # sample up to 30 tokens\n",
    "# torch.manual_seed(0)  # doctest: +IGNORE_RESULT\n",
    "# outputs = custom_model.generate(input_ids=input_ids, do_sample=True, max_length=30)\n",
    "\n",
    "# pred_custom=tokenizer.batch_decode(outputs, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
    "# # ['Today I believe we can finally get rid of discrimination,\" said Rep. Mark Pocan (D-Wis.).\\n\\n\"Just look at the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# p_count=50\n",
    "psi,pei=50,55\n",
    "prompts = ftrain_data.loc[psi:pei,['input','instruction']].values.tolist()\n",
    "t_prompts=[]\n",
    "for p in prompts:\n",
    "    context=str(p[0]).replace(r\"\\n\",'.')\n",
    "    question=p[1]\n",
    "    t_prompts+=[f\"paragraph: {context}\\n\\n Answer the following question from the paragraph, if you cant find the answer say unknown : Question: {question}\"]\n",
    "    # t_prompts+=[f\"input: {context}\\n\\ninstruction: {question}\"]\n",
    "\n",
    "prompts=t_prompts\n",
    "# prompts = ftrain_data.loc[:p_count,['input','instruction']].values.tolist()\n",
    "# pprint(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/u131168/mh_one_api/model/predict.ipynb Cell 9\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bmh_one_api/home/u131168/mh_one_api/model/predict.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# sample up to 30 tokens\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bmh_one_api/home/u131168/mh_one_api/model/predict.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m torch\u001b[39m.\u001b[39mmanual_seed(\u001b[39m0\u001b[39m)  \u001b[39m# doctest: +IGNORE_RESULT\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://tunnel%2Bmh_one_api/home/u131168/mh_one_api/model/predict.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m outputs \u001b[39m=\u001b[39m custom_model\u001b[39m.\u001b[39;49mgenerate(input_ids\u001b[39m=\u001b[39;49minput_ids, do_sample\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, max_length\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bmh_one_api/home/u131168/mh_one_api/model/predict.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m pred_custom\u001b[39m=\u001b[39mtokenizer\u001b[39m.\u001b[39mbatch_decode(outputs, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/peft/peft_model.py:1190\u001b[0m, in \u001b[0;36mPeftModelForSeq2SeqLM.generate\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1189\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m peft_config\u001b[39m.\u001b[39mis_prompt_learning:\n\u001b[0;32m-> 1190\u001b[0m         outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_model\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1192\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m kwargs:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.10/site-packages/transformers/generation/utils.py:1652\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1644\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1645\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1646\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1647\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1648\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1649\u001b[0m     )\n\u001b[1;32m   1651\u001b[0m     \u001b[39m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample(\n\u001b[1;32m   1653\u001b[0m         input_ids,\n\u001b[1;32m   1654\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1655\u001b[0m         logits_warper\u001b[39m=\u001b[39;49mlogits_warper,\n\u001b[1;32m   1656\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1657\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1658\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1659\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1660\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1661\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1662\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1663\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1664\u001b[0m     )\n\u001b[1;32m   1666\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1667\u001b[0m     \u001b[39m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1668\u001b[0m     beam_scorer \u001b[39m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1669\u001b[0m         batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1670\u001b[0m         num_beams\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1675\u001b[0m         max_length\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mmax_length,\n\u001b[1;32m   1676\u001b[0m     )\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.10/site-packages/transformers/generation/utils.py:2734\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2731\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2733\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2734\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2735\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2736\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2737\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2738\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2739\u001b[0m )\n\u001b[1;32m   2741\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2742\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1746\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         decoder_attention_mask \u001b[39m=\u001b[39m decoder_attention_mask\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39mfirst_device)\n\u001b[1;32m   1745\u001b[0m \u001b[39m# Decode\u001b[39;00m\n\u001b[0;32m-> 1746\u001b[0m decoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(\n\u001b[1;32m   1747\u001b[0m     input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[1;32m   1748\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[1;32m   1749\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[1;32m   1750\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1751\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m   1752\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1753\u001b[0m     head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[1;32m   1754\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[1;32m   1755\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1756\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1757\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1758\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1759\u001b[0m )\n\u001b[1;32m   1761\u001b[0m sequence_output \u001b[39m=\u001b[39m decoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1763\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1123\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1110\u001b[0m     layer_outputs \u001b[39m=\u001b[39m checkpoint(\n\u001b[1;32m   1111\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m   1112\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1120\u001b[0m         \u001b[39mNone\u001b[39;00m,  \u001b[39m# past_key_value is always None with gradient checkpointing\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     )\n\u001b[1;32m   1122\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1123\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m   1124\u001b[0m         hidden_states,\n\u001b[1;32m   1125\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   1126\u001b[0m         position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[1;32m   1127\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1128\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   1129\u001b[0m         encoder_decoder_position_bias\u001b[39m=\u001b[39;49mencoder_decoder_position_bias,\n\u001b[1;32m   1130\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[1;32m   1131\u001b[0m         cross_attn_layer_head_mask\u001b[39m=\u001b[39;49mcross_attn_layer_head_mask,\n\u001b[1;32m   1132\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m   1133\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1134\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1135\u001b[0m     )\n\u001b[1;32m   1137\u001b[0m \u001b[39m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[39m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:725\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    723\u001b[0m     query_length \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 725\u001b[0m cross_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer[\u001b[39m1\u001b[39;49m](\n\u001b[1;32m    726\u001b[0m     hidden_states,\n\u001b[1;32m    727\u001b[0m     key_value_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    728\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    729\u001b[0m     position_bias\u001b[39m=\u001b[39;49mencoder_decoder_position_bias,\n\u001b[1;32m    730\u001b[0m     layer_head_mask\u001b[39m=\u001b[39;49mcross_attn_layer_head_mask,\n\u001b[1;32m    731\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mcross_attn_past_key_value,\n\u001b[1;32m    732\u001b[0m     query_length\u001b[39m=\u001b[39;49mquery_length,\n\u001b[1;32m    733\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    734\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    735\u001b[0m )\n\u001b[1;32m    736\u001b[0m hidden_states \u001b[39m=\u001b[39m cross_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    738\u001b[0m \u001b[39m# clamp inf values to enable fp16 training\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:636\u001b[0m, in \u001b[0;36mT5LayerCrossAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, query_length, output_attentions)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    624\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    625\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    633\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    634\u001b[0m ):\n\u001b[1;32m    635\u001b[0m     normed_hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 636\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mEncDecAttention(\n\u001b[1;32m    637\u001b[0m         normed_hidden_states,\n\u001b[1;32m    638\u001b[0m         mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    639\u001b[0m         key_value_states\u001b[39m=\u001b[39;49mkey_value_states,\n\u001b[1;32m    640\u001b[0m         position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[1;32m    641\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[1;32m    642\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    643\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    644\u001b[0m         query_length\u001b[39m=\u001b[39;49mquery_length,\n\u001b[1;32m    645\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    646\u001b[0m     )\n\u001b[1;32m    647\u001b[0m     layer_output \u001b[39m=\u001b[39m hidden_states \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(attention_output[\u001b[39m0\u001b[39m])\n\u001b[1;32m    648\u001b[0m     outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m attention_output[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:532\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    527\u001b[0m value_states \u001b[39m=\u001b[39m project(\n\u001b[1;32m    528\u001b[0m     hidden_states, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv, key_value_states, past_key_value[\u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    529\u001b[0m )\n\u001b[1;32m    531\u001b[0m \u001b[39m# compute scores\u001b[39;00m\n\u001b[0;32m--> 532\u001b[0m scores \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(\n\u001b[1;32m    533\u001b[0m     query_states, key_states\u001b[39m.\u001b[39;49mtranspose(\u001b[39m3\u001b[39;49m, \u001b[39m2\u001b[39;49m)\n\u001b[1;32m    534\u001b[0m )  \u001b[39m# equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9\u001b[39;00m\n\u001b[1;32m    536\u001b[0m \u001b[39mif\u001b[39;00m position_bias \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    537\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_relative_attention_bias:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(prompts, return_tensors=\"pt\" ,padding=True,truncation=True, max_length=512).input_ids\n",
    "# sample up to 30 tokens\n",
    "torch.manual_seed(0)  # doctest: +IGNORE_RESULT\n",
    "outputs = custom_model.generate(input_ids=input_ids, do_sample=True, max_length=20)\n",
    "pred_custom=tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Grief',\n",
       " 'i.',\n",
       " 'December 17',\n",
       " 'To ask something to his grandmother',\n",
       " 'he was composed',\n",
       " 'it was fun',\n",
       " 'Madison Square Garden',\n",
       " 'Lightfoot',\n",
       " 'leave',\n",
       " 'i.',\n",
       " 'CSKA Moscow',\n",
       " 'Germanic',\n",
       " 'Grand Cayman, Cayman Brac and Little Cayman',\n",
       " 'Shahadah',\n",
       " 'What the time was?',\n",
       " 'Founded in 1565 by the Portuguese,',\n",
       " '3',\n",
       " '(ii)',\n",
       " 'irremediable',\n",
       " 'Zahra Clare Baker',\n",
       " 'no',\n",
       " 'South Waziristan province',\n",
       " 'the Secretary of State',\n",
       " 'Simón Bolvar',\n",
       " 'Josiah Graham,',\n",
       " 'Finland',\n",
       " 'Knights of the Bath',\n",
       " 'Tom to look past the fountain',\n",
       " 'Whoa!',\n",
       " 'Vancouver is consistently named as one of the top five worldwide cities for livability and quality of',\n",
       " 'none are known to be related',\n",
       " 'A).',\n",
       " 'The searchlight',\n",
       " 'He is devastated and horrified by the death of the teen',\n",
       " 'yes',\n",
       " 'I.',\n",
       " 'He was completely exhausted, completely dehydrated, suffered significant weight loss, somewhere up to 50%.',\n",
       " 'I broke one leg and broke the ankle in my other leg',\n",
       " '[B].',\n",
       " \"John's mother.\",\n",
       " 'It took her a few hours to pick enough apples to make dozens of pies.',\n",
       " 'the Greek  \"to choke\"',\n",
       " 'No, Larry is one of their best chums',\n",
       " 'Nairo Quintana',\n",
       " 'Libby Little',\n",
       " 'Manhattan',\n",
       " '(iii).',\n",
       " 'adapts for film and television',\n",
       " 'Repblica Bolivariana de Venezuela',\n",
       " 'C,D,and E',\n",
       " 'Berlin']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import math\n",
    "# input_ids = tokenizer(prompts, return_tensors=\"pt\" ,padding=True,truncation=True, max_length=512).input_ids\n",
    "# # input_ids=[input_ids]\n",
    "# # sample up to 30 tokens\n",
    "# torch.manual_seed(0)  # doctest: +IGNORE_RESULT\n",
    "# outputs = ft5_model.generate(input_ids=input_ids, do_sample=True, max_length=20,)\n",
    "# # print(sum(list(sum(outputs.scores))))\n",
    "# pred_ft5=tokenizer.batch_decode(outputs, skip_special_tokens=True,)\n",
    "# # pprint(prompts[5])\n",
    "# pred_ft5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred_custom' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/u131168/mh_one_api/model/predict.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://tunnel%2Bmh_one_api/home/u131168/mh_one_api/model/predict.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m pred_custom_df\u001b[39m=\u001b[39mpd\u001b[39m.\u001b[39mDataFrame(pd\u001b[39m.\u001b[39mSeries(pred_custom,name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpred_custom\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bmh_one_api/home/u131168/mh_one_api/model/predict.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# pred_ft5_df=pd.DataFrame(pd.Series(pred_ft5,name=\"pred_ft5\"))\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bmh_one_api/home/u131168/mh_one_api/model/predict.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m temp_df\u001b[39m=\u001b[39mpd\u001b[39m.\u001b[39mconcat([ftrain_data\u001b[39m.\u001b[39miloc[psi:pei]\u001b[39m.\u001b[39mreset_index(),train_data\u001b[39m.\u001b[39miloc[psi:pei,\u001b[39m4\u001b[39m]\u001b[39m.\u001b[39mreset_index(),pred_custom_df,],axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pred_custom' is not defined"
     ]
    }
   ],
   "source": [
    "pred_custom_df=pd.DataFrame(pd.Series(pred_custom,name=\"pred_custom\"))\n",
    "# pred_ft5_df=pd.DataFrame(pd.Series(pred_ft5,name=\"pred_ft5\"))\n",
    "temp_df=pd.concat([ftrain_data.iloc[psi:pei].reset_index(),train_data.iloc[psi:pei,4].reset_index(),pred_custom_df,],axis=1)\n",
    "temp_df\n",
    "# pprint(temp_df.iloc[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Story</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHAPTER I \\n\\nON THE HOUSEBOAT \\n\\n\"Say, Tom, ...</td>\n",
       "      <td>What was Sam's mood?</td>\n",
       "      <td>Anxious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Uralic languages (; sometimes called Urali...</td>\n",
       "      <td>By how many?</td>\n",
       "      <td>125 million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The University of Wisconsin–Madison (also know...</td>\n",
       "      <td>How many schools and colleges does it have?</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(CNN) -- The unexpected resignation of David P...</td>\n",
       "      <td>What happened?</td>\n",
       "      <td>David Petraeus remained stoically aloof as a s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHAPTER THIRTY-TWO \\n\\nTENDER TROUBLES \\n\\n\"Jo...</td>\n",
       "      <td>Why does Mrs. March not force her children's c...</td>\n",
       "      <td>She wants to wait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The New York Yankees are an American professio...</td>\n",
       "      <td>When did the team start playing in Yankee Stad...</td>\n",
       "      <td>1923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>South Ossetia () is a partially recognised sta...</td>\n",
       "      <td>Who does Russia not allow to enter there?</td>\n",
       "      <td>European Union</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(CNN) -- The megayacht that Steve Jobs commiss...</td>\n",
       "      <td>How much money did Steve Jobs commission for t...</td>\n",
       "      <td>100-million-euro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CHAPTER XV \\n\\nNow that Gordon was gone, at an...</td>\n",
       "      <td>Does he think the reason she gave are conflict...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A high-profile murder case involving one of Am...</td>\n",
       "      <td>For the murder of whom?</td>\n",
       "      <td>__</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CHAPTER VII \\n\\nTHE END OF THE TERM \\n\\n\"What ...</td>\n",
       "      <td>Where does Gabe live?</td>\n",
       "      <td>the school</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(CNN) -- He was the soccer referee known as \"g...</td>\n",
       "      <td>Was anyone else caught?</td>\n",
       "      <td>Wan Daxue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CHAPTER I \\n\\nTHE BOYS OF OAK HALL \\n\\n\"Hello,...</td>\n",
       "      <td>who was resting on one of the beds?</td>\n",
       "      <td>Shadow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>JERUSALEM (CNN) -- The world knows her as the ...</td>\n",
       "      <td>Where?</td>\n",
       "      <td>Israel's Chabad movement has set up a fund to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>A telescope is an optical instrument that aids...</td>\n",
       "      <td>What year?</td>\n",
       "      <td>1608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>CHAPTER XIII \\n\\nLOST IN THE SNOW \\n\\n\"The ice...</td>\n",
       "      <td>What did Hans call Tom?</td>\n",
       "      <td>a goot feller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>CHAPTER L \\n\\nThe Duke's Arguments \\n\\nThe Duk...</td>\n",
       "      <td>What did Lady Cantrip think of Mary's character?</td>\n",
       "      <td>Mary has great firmness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Joey felt the very first rain drop hit his hat...</td>\n",
       "      <td>What was he doing at the time?</td>\n",
       "      <td>watching out the window</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Marietta, Georgia (CNN) -- Whether the prosecu...</td>\n",
       "      <td>Who is the defendant?</td>\n",
       "      <td>Justin Ross Harris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Piedmont ( ; , ; Piedmontese, Occitan and ; ) ...</td>\n",
       "      <td>Which is first?</td>\n",
       "      <td>Piedmont</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>(CNN)A teen couple from Kentucky was arrested ...</td>\n",
       "      <td>What had kin of the female done?</td>\n",
       "      <td>reported her missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Chapter 9 \\n\\nChivalry or Villainy \\n\\nFrom he...</td>\n",
       "      <td>Who did he hang out with?</td>\n",
       "      <td>savage beasts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>(CNN) -- A former Utah policeman is a suspect ...</td>\n",
       "      <td>What was Smiths' profession?</td>\n",
       "      <td>policeman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>(CNN) -- After months of speculation, one of F...</td>\n",
       "      <td>How many did they lose?</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>The University of Illinois at Urbana–Champaign...</td>\n",
       "      <td>How many different things are there for studying?</td>\n",
       "      <td>more than 150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Saturday Night Live (abbreviated as SNL) is an...</td>\n",
       "      <td>Does it have an abbreviation?</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>CHAPTER XXVIII \\n\\nSPENNIE'S HOUR OF CLEAR VIS...</td>\n",
       "      <td>Had he stopped smiling by then?</td>\n",
       "      <td>ballroom's music had died away.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Elizabeth I (7 September 1533 – 24 March 1603)...</td>\n",
       "      <td>until when?</td>\n",
       "      <td>her death</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>East Timor () or Timor-Leste (; Tetum: \"Timór ...</td>\n",
       "      <td>Who did it get freedom from?</td>\n",
       "      <td>Portugal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>(CNN) -- Two Amish girls, who were apparently ...</td>\n",
       "      <td>where were they abducted from?</td>\n",
       "      <td>a roadside farm stand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Kiev, Ukraine (CNN) -- Ukraine and Russia plan...</td>\n",
       "      <td>When?</td>\n",
       "      <td>Tuesday,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>(CNN) -- Members of the international communit...</td>\n",
       "      <td>What did U.S. Vice President Joe Biden express...</td>\n",
       "      <td>the Iranian government had suppressed crowds a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Tristan da Cunha /ˈtrɪstən də ˈkuːnjə/, colloq...</td>\n",
       "      <td>Are all of them populated?</td>\n",
       "      <td>uninhabited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Singing competition shows like \"American Idol\"...</td>\n",
       "      <td>What could Britney Spears do to reignite the l...</td>\n",
       "      <td>showing off her personality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>CHAPTER VIII \\n\\nD'AGUILAR SPEAKS \\n\\n\"Losses?...</td>\n",
       "      <td>Did he admit knowing about the other man's tro...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Chapter V. Mohun Appears For The Last Time In ...</td>\n",
       "      <td>Where?</td>\n",
       "      <td>Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>I'd had the piggy bank for a long time. So lon...</td>\n",
       "      <td>Who told her the story about it?</td>\n",
       "      <td>Her Aunt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>A police cruiser draped in black banners and t...</td>\n",
       "      <td>How long is the investigation into the acciden...</td>\n",
       "      <td>could take months</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>The National Aeronautics and Space Administrat...</td>\n",
       "      <td>Did it lead the Moon landing?</td>\n",
       "      <td>NASA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Koninklijke Philips N.V. (Koninklijke Philips ...</td>\n",
       "      <td>where?</td>\n",
       "      <td>Amsterdam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>CHAPTER LXV - MISS LONGESTAFFE WRITES HOME \\n\\...</td>\n",
       "      <td>with whom?</td>\n",
       "      <td>Lady Monogram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Chapter VI. -- THE LITTLE DRUMMER. \\n\\nThis Si...</td>\n",
       "      <td>which one was the lead one?</td>\n",
       "      <td>Count Fink von Finkenstein</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>CHAPTER THIRTY TWO. \\n\\nTOUCHES ON LOVE AND ON...</td>\n",
       "      <td>What is he favored by?</td>\n",
       "      <td>Fortune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Tampa () is a major city in, and the county se...</td>\n",
       "      <td>What happened to the native societies when the...</td>\n",
       "      <td>introduced European diseases which brought the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>CHAPTER XIX \\n\\nA letter, edged with black, an...</td>\n",
       "      <td>How was it decorated?</td>\n",
       "      <td>By the mourning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>CHAPTER XX. \\n\\nHOW CAPTAIN USSHER SUCCEEDED. ...</td>\n",
       "      <td>Who left after breakfast?</td>\n",
       "      <td>Feemy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>CHAPTER EIGHTEEN. \\n\\nANXIOUS TIMES--A SEARCH ...</td>\n",
       "      <td>What made Freydissa nicer?</td>\n",
       "      <td>by the sight of the child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>The Whigs were a political faction and then a ...</td>\n",
       "      <td>When did they start?</td>\n",
       "      <td>Between the 1680s and 1850s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>CHAPTER XVIII \\n\\nTHE HAND IN THE WATER \\n\\nSc...</td>\n",
       "      <td>what kind of blood does he have?</td>\n",
       "      <td>Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>(CNN) -- \"I don't know the ins and outs of his...</td>\n",
       "      <td>How many shows were on Noel Gallagher's tour f...</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Story  \\\n",
       "0   CHAPTER I \\n\\nON THE HOUSEBOAT \\n\\n\"Say, Tom, ...   \n",
       "1   The Uralic languages (; sometimes called Urali...   \n",
       "2   The University of Wisconsin–Madison (also know...   \n",
       "3   (CNN) -- The unexpected resignation of David P...   \n",
       "4   CHAPTER THIRTY-TWO \\n\\nTENDER TROUBLES \\n\\n\"Jo...   \n",
       "5   The New York Yankees are an American professio...   \n",
       "6   South Ossetia () is a partially recognised sta...   \n",
       "7   (CNN) -- The megayacht that Steve Jobs commiss...   \n",
       "8   CHAPTER XV \\n\\nNow that Gordon was gone, at an...   \n",
       "9   A high-profile murder case involving one of Am...   \n",
       "10  CHAPTER VII \\n\\nTHE END OF THE TERM \\n\\n\"What ...   \n",
       "11  (CNN) -- He was the soccer referee known as \"g...   \n",
       "12  CHAPTER I \\n\\nTHE BOYS OF OAK HALL \\n\\n\"Hello,...   \n",
       "13  JERUSALEM (CNN) -- The world knows her as the ...   \n",
       "14  A telescope is an optical instrument that aids...   \n",
       "15  CHAPTER XIII \\n\\nLOST IN THE SNOW \\n\\n\"The ice...   \n",
       "16  CHAPTER L \\n\\nThe Duke's Arguments \\n\\nThe Duk...   \n",
       "17  Joey felt the very first rain drop hit his hat...   \n",
       "18  Marietta, Georgia (CNN) -- Whether the prosecu...   \n",
       "19  Piedmont ( ; , ; Piedmontese, Occitan and ; ) ...   \n",
       "20  (CNN)A teen couple from Kentucky was arrested ...   \n",
       "21  Chapter 9 \\n\\nChivalry or Villainy \\n\\nFrom he...   \n",
       "22  (CNN) -- A former Utah policeman is a suspect ...   \n",
       "23  (CNN) -- After months of speculation, one of F...   \n",
       "24  The University of Illinois at Urbana–Champaign...   \n",
       "25  Saturday Night Live (abbreviated as SNL) is an...   \n",
       "26  CHAPTER XXVIII \\n\\nSPENNIE'S HOUR OF CLEAR VIS...   \n",
       "27  Elizabeth I (7 September 1533 – 24 March 1603)...   \n",
       "28  East Timor () or Timor-Leste (; Tetum: \"Timór ...   \n",
       "29  (CNN) -- Two Amish girls, who were apparently ...   \n",
       "30  Kiev, Ukraine (CNN) -- Ukraine and Russia plan...   \n",
       "31  (CNN) -- Members of the international communit...   \n",
       "32  Tristan da Cunha /ˈtrɪstən də ˈkuːnjə/, colloq...   \n",
       "33  Singing competition shows like \"American Idol\"...   \n",
       "34  CHAPTER VIII \\n\\nD'AGUILAR SPEAKS \\n\\n\"Losses?...   \n",
       "35  Chapter V. Mohun Appears For The Last Time In ...   \n",
       "36  I'd had the piggy bank for a long time. So lon...   \n",
       "37  A police cruiser draped in black banners and t...   \n",
       "38  The National Aeronautics and Space Administrat...   \n",
       "39  Koninklijke Philips N.V. (Koninklijke Philips ...   \n",
       "40  CHAPTER LXV - MISS LONGESTAFFE WRITES HOME \\n\\...   \n",
       "41  Chapter VI. -- THE LITTLE DRUMMER. \\n\\nThis Si...   \n",
       "42  CHAPTER THIRTY TWO. \\n\\nTOUCHES ON LOVE AND ON...   \n",
       "43  Tampa () is a major city in, and the county se...   \n",
       "44  CHAPTER XIX \\n\\nA letter, edged with black, an...   \n",
       "45  CHAPTER XX. \\n\\nHOW CAPTAIN USSHER SUCCEEDED. ...   \n",
       "46  CHAPTER EIGHTEEN. \\n\\nANXIOUS TIMES--A SEARCH ...   \n",
       "47  The Whigs were a political faction and then a ...   \n",
       "48  CHAPTER XVIII \\n\\nTHE HAND IN THE WATER \\n\\nSc...   \n",
       "49  (CNN) -- \"I don't know the ins and outs of his...   \n",
       "\n",
       "                                             Question  \\\n",
       "0                                What was Sam's mood?   \n",
       "1                                        By how many?   \n",
       "2         How many schools and colleges does it have?   \n",
       "3                                      What happened?   \n",
       "4   Why does Mrs. March not force her children's c...   \n",
       "5   When did the team start playing in Yankee Stad...   \n",
       "6           Who does Russia not allow to enter there?   \n",
       "7   How much money did Steve Jobs commission for t...   \n",
       "8   Does he think the reason she gave are conflict...   \n",
       "9                             For the murder of whom?   \n",
       "10                              Where does Gabe live?   \n",
       "11                            Was anyone else caught?   \n",
       "12                who was resting on one of the beds?   \n",
       "13                                             Where?   \n",
       "14                                         What year?   \n",
       "15                            What did Hans call Tom?   \n",
       "16   What did Lady Cantrip think of Mary's character?   \n",
       "17                     What was he doing at the time?   \n",
       "18                              Who is the defendant?   \n",
       "19                                    Which is first?   \n",
       "20                   What had kin of the female done?   \n",
       "21                          Who did he hang out with?   \n",
       "22                       What was Smiths' profession?   \n",
       "23                            How many did they lose?   \n",
       "24  How many different things are there for studying?   \n",
       "25                      Does it have an abbreviation?   \n",
       "26                    Had he stopped smiling by then?   \n",
       "27                                        until when?   \n",
       "28                       Who did it get freedom from?   \n",
       "29                     where were they abducted from?   \n",
       "30                                              When?   \n",
       "31  What did U.S. Vice President Joe Biden express...   \n",
       "32                         Are all of them populated?   \n",
       "33  What could Britney Spears do to reignite the l...   \n",
       "34  Did he admit knowing about the other man's tro...   \n",
       "35                                             Where?   \n",
       "36                   Who told her the story about it?   \n",
       "37  How long is the investigation into the acciden...   \n",
       "38                      Did it lead the Moon landing?   \n",
       "39                                             where?   \n",
       "40                                         with whom?   \n",
       "41                        which one was the lead one?   \n",
       "42                             What is he favored by?   \n",
       "43  What happened to the native societies when the...   \n",
       "44                              How was it decorated?   \n",
       "45                          Who left after breakfast?   \n",
       "46                         What made Freydissa nicer?   \n",
       "47                               When did they start?   \n",
       "48                   what kind of blood does he have?   \n",
       "49  How many shows were on Noel Gallagher's tour f...   \n",
       "\n",
       "                                               Answer  \n",
       "0                                             Anxious  \n",
       "1                                         125 million  \n",
       "2                                                  20  \n",
       "3   David Petraeus remained stoically aloof as a s...  \n",
       "4                                   She wants to wait  \n",
       "5                                                1923  \n",
       "6                                      European Union  \n",
       "7                                    100-million-euro  \n",
       "8                                                 yes  \n",
       "9                                                  __  \n",
       "10                                         the school  \n",
       "11                                          Wan Daxue  \n",
       "12                                             Shadow  \n",
       "13  Israel's Chabad movement has set up a fund to ...  \n",
       "14                                               1608  \n",
       "15                                      a goot feller  \n",
       "16                            Mary has great firmness  \n",
       "17                            watching out the window  \n",
       "18                                 Justin Ross Harris  \n",
       "19                                           Piedmont  \n",
       "20                               reported her missing  \n",
       "21                                      savage beasts  \n",
       "22                                          policeman  \n",
       "23                                                 44  \n",
       "24                                      more than 150  \n",
       "25                                                yes  \n",
       "26                    ballroom's music had died away.  \n",
       "27                                          her death  \n",
       "28                                           Portugal  \n",
       "29                              a roadside farm stand  \n",
       "30                                           Tuesday,  \n",
       "31  the Iranian government had suppressed crowds a...  \n",
       "32                                        uninhabited  \n",
       "33                        showing off her personality  \n",
       "34                                                yes  \n",
       "35                                              Paris  \n",
       "36                                           Her Aunt  \n",
       "37                                  could take months  \n",
       "38                                               NASA  \n",
       "39                                          Amsterdam  \n",
       "40                                      Lady Monogram  \n",
       "41                         Count Fink von Finkenstein  \n",
       "42                                            Fortune  \n",
       "43  introduced European diseases which brought the...  \n",
       "44                                    By the mourning  \n",
       "45                                              Feemy  \n",
       "46                          by the sight of the child  \n",
       "47                        Between the 1680s and 1850s  \n",
       "48                                             Indian  \n",
       "49                                                 81  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pred_custom_df=pd.DataFrame(pd.Series(pred_custom,name=\"pred_custom\"))\n",
    "# pred_ft5_df=pd.DataFrame(pd.Series(pred_ft5,name=\"pred_ft5\"))\n",
    "pred_ft5_df=pd.read_csv(\"/home/u131168/mh_one_api/data/sub_ft5.csv\")\n",
    "temp_df=pd.concat([test_data.iloc[:p_count],pred_ft5_df[:p_count]],axis=1)\n",
    "temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Story</th>\n",
       "      <th>Question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHAPTER I \\n\\nON THE HOUSEBOAT \\n\\n\"Say, Tom, ...</td>\n",
       "      <td>What was Sam's mood?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Uralic languages (; sometimes called Urali...</td>\n",
       "      <td>By how many?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The University of Wisconsin–Madison (also know...</td>\n",
       "      <td>How many schools and colleges does it have?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(CNN) -- The unexpected resignation of David P...</td>\n",
       "      <td>What happened?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHAPTER THIRTY-TWO \\n\\nTENDER TROUBLES \\n\\n\"Jo...</td>\n",
       "      <td>Why does Mrs. March not force her children's c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28543</th>\n",
       "      <td>CHAPTER III. \\n\\n\"Nice customs curt'sy to grea...</td>\n",
       "      <td>Who was the Archbishop of Toledo that was of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28544</th>\n",
       "      <td>CHAPTER XI. \\n\\nIn the little dining-room of t...</td>\n",
       "      <td>Who lived there?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28545</th>\n",
       "      <td>Chapter XXX. \\n\\n\"I shall go on through all et...</td>\n",
       "      <td>is he fair?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28546</th>\n",
       "      <td>Chapter XXXIV \\n\\nViolence \\n\\n\\n\\nIt had been...</td>\n",
       "      <td>Where was he to stay until this trip?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28547</th>\n",
       "      <td>Be home by dinnertime, Eric's mother said as h...</td>\n",
       "      <td>what reptile did his friend bring?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28548 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Story  \\\n",
       "0      CHAPTER I \\n\\nON THE HOUSEBOAT \\n\\n\"Say, Tom, ...   \n",
       "1      The Uralic languages (; sometimes called Urali...   \n",
       "2      The University of Wisconsin–Madison (also know...   \n",
       "3      (CNN) -- The unexpected resignation of David P...   \n",
       "4      CHAPTER THIRTY-TWO \\n\\nTENDER TROUBLES \\n\\n\"Jo...   \n",
       "...                                                  ...   \n",
       "28543  CHAPTER III. \\n\\n\"Nice customs curt'sy to grea...   \n",
       "28544  CHAPTER XI. \\n\\nIn the little dining-room of t...   \n",
       "28545  Chapter XXX. \\n\\n\"I shall go on through all et...   \n",
       "28546  Chapter XXXIV \\n\\nViolence \\n\\n\\n\\nIt had been...   \n",
       "28547  Be home by dinnertime, Eric's mother said as h...   \n",
       "\n",
       "                                                Question  \n",
       "0                                   What was Sam's mood?  \n",
       "1                                           By how many?  \n",
       "2            How many schools and colleges does it have?  \n",
       "3                                         What happened?  \n",
       "4      Why does Mrs. March not force her children's c...  \n",
       "...                                                  ...  \n",
       "28543  Who was the Archbishop of Toledo that was of t...  \n",
       "28544                                   Who lived there?  \n",
       "28545                                        is he fair?  \n",
       "28546              Where was he to stay until this trip?  \n",
       "28547                 what reptile did his friend bring?  \n",
       "\n",
       "[28548 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anxious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125 million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>David Petraeus remained stoically aloof as a s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>She wants to wait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28543</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28544</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28545</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28546</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28547</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28548 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Answer\n",
       "0                                                Anxious\n",
       "1                                            125 million\n",
       "2                                                     20\n",
       "3      David Petraeus remained stoically aloof as a s...\n",
       "4                                      She wants to wait\n",
       "...                                                  ...\n",
       "28543                                                  0\n",
       "28544                                                  0\n",
       "28545                                                  0\n",
       "28546                                                  0\n",
       "28547                                                  0\n",
       "\n",
       "[28548 rows x 1 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temList=[0 for i in range(28548-len(res))]\n",
    "submission_data=pd.Series(res+temList,name=\"Answer\")\n",
    "submission_data=pd.DataFrame(submission_data)\n",
    "submission_data.to_csv(\"/home/u131168/mh_one_api/data/submissionflant5base.csv\",index=False)\n",
    "submission_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
