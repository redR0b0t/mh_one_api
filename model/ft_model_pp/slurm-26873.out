got current job name=fti6
new job name=fti7
new job created with id: 26874
starting fine tuning model
Requirement already satisfied: datasets in /home/u131168/.local/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (2.14.5)
Requirement already satisfied: torch in /home/u131168/.local/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (2.0.1)
Requirement already satisfied: transformers>=4.32.0 in /opt/miniconda/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (4.35.0.dev0)
Requirement already satisfied: sentencepiece in /home/u131168/.local/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (0.1.99)
Requirement already satisfied: peft in /home/u131168/.local/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (0.5.0)
Requirement already satisfied: evaluate in /home/u131168/.local/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (0.4.0)
Requirement already satisfied: nltk in /home/u131168/.local/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (3.8.1)
Requirement already satisfied: rouge_score in /home/u131168/.local/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (0.1.2)
Requirement already satisfied: einops in /home/u131168/.local/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (0.6.1)
Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/u131168/.local/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (0.3.7)
Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /home/u131168/.local/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (0.16.4)
Requirement already satisfied: pandas in /home/u131168/.local/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (2.1.0)
Requirement already satisfied: xxhash in /home/u131168/.local/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (3.3.0)
Requirement already satisfied: tqdm>=4.62.1 in /opt/miniconda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (4.65.0)
Requirement already satisfied: packaging in /opt/miniconda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (23.0)
Requirement already satisfied: numpy>=1.17 in /home/u131168/.local/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (1.26.0)
Requirement already satisfied: aiohttp in /home/u131168/.local/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (3.8.5)
Requirement already satisfied: pyarrow>=8.0.0 in /home/u131168/.local/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (13.0.0)
Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /home/u131168/.local/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (2023.6.0)
Requirement already satisfied: pyyaml>=5.1 in /home/u131168/.local/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (6.0.1)
Requirement already satisfied: multiprocess in /home/u131168/.local/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (0.70.15)
Requirement already satisfied: requests>=2.19.0 in /opt/miniconda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (2.31.0)
Requirement already satisfied: typing-extensions in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (4.8.0)
Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (11.10.3.66)
Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (11.4.0.1)
Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (11.7.99)
Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (11.7.101)
Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (11.7.4.91)
Requirement already satisfied: triton==2.0.0 in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (2.0.0)
Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (11.7.99)
Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (8.5.0.96)
Requirement already satisfied: jinja2 in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (3.1.2)
Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (10.9.0.58)
Requirement already satisfied: sympy in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (1.12)
Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (10.2.10.91)
Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (2.14.3)
Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (11.7.91)
Requirement already satisfied: filelock in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (3.12.4)
Requirement already satisfied: networkx in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (3.1)
Requirement already satisfied: setuptools in /opt/miniconda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->-r requirements.txt (line 2)) (65.6.3)
Requirement already satisfied: wheel in /opt/miniconda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->-r requirements.txt (line 2)) (0.38.4)
Requirement already satisfied: cmake in /home/u131168/.local/lib/python3.10/site-packages (from triton==2.0.0->torch->-r requirements.txt (line 2)) (3.27.5)
Requirement already satisfied: lit in /home/u131168/.local/lib/python3.10/site-packages (from triton==2.0.0->torch->-r requirements.txt (line 2)) (16.0.6)
Requirement already satisfied: safetensors>=0.3.1 in /home/u131168/.local/lib/python3.10/site-packages (from transformers>=4.32.0->-r requirements.txt (line 3)) (0.3.3)
Requirement already satisfied: tokenizers<0.15,>=0.14 in /home/u131168/.local/lib/python3.10/site-packages (from transformers>=4.32.0->-r requirements.txt (line 3)) (0.14.0)
Requirement already satisfied: regex!=2019.12.17 in /home/u131168/.local/lib/python3.10/site-packages (from transformers>=4.32.0->-r requirements.txt (line 3)) (2023.8.8)
Requirement already satisfied: accelerate in /home/u131168/.local/lib/python3.10/site-packages (from peft->-r requirements.txt (line 5)) (0.23.0)
Requirement already satisfied: psutil in /home/u131168/.local/lib/python3.10/site-packages (from peft->-r requirements.txt (line 5)) (5.9.5)
Requirement already satisfied: responses<0.19 in /home/u131168/.local/lib/python3.10/site-packages (from evaluate->-r requirements.txt (line 6)) (0.18.0)
Requirement already satisfied: click in /home/u131168/.local/lib/python3.10/site-packages (from nltk->-r requirements.txt (line 7)) (8.1.7)
Requirement already satisfied: joblib in /home/u131168/.local/lib/python3.10/site-packages (from nltk->-r requirements.txt (line 7)) (1.3.2)
Requirement already satisfied: absl-py in /home/u131168/.local/lib/python3.10/site-packages (from rouge_score->-r requirements.txt (line 8)) (2.0.0)
Requirement already satisfied: six>=1.14.0 in /opt/miniconda/lib/python3.10/site-packages (from rouge_score->-r requirements.txt (line 8)) (1.16.0)
Requirement already satisfied: aiosignal>=1.1.2 in /home/u131168/.local/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.3.1)
Requirement already satisfied: multidict<7.0,>=4.5 in /home/u131168/.local/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (6.0.4)
Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/miniconda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (2.0.4)
Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/u131168/.local/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (4.0.3)
Requirement already satisfied: yarl<2.0,>=1.0 in /home/u131168/.local/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.9.2)
Requirement already satisfied: frozenlist>=1.1.1 in /home/u131168/.local/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.4.0)
Requirement already satisfied: attrs>=17.3.0 in /home/u131168/.local/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (23.1.0)
Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda/lib/python3.10/site-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 1)) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda/lib/python3.10/site-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 1)) (1.26.15)
Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda/lib/python3.10/site-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 1)) (2023.5.7)
Requirement already satisfied: MarkupSafe>=2.0 in /home/u131168/.local/lib/python3.10/site-packages (from jinja2->torch->-r requirements.txt (line 2)) (2.1.3)
Requirement already satisfied: pytz>=2020.1 in /home/u131168/.local/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2023.3.post1)
Requirement already satisfied: python-dateutil>=2.8.2 in /home/u131168/.local/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2.8.2)
Requirement already satisfied: tzdata>=2022.1 in /home/u131168/.local/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2023.3)
Requirement already satisfied: mpmath>=0.19 in /home/u131168/.local/lib/python3.10/site-packages (from sympy->torch->-r requirements.txt (line 2)) (1.3.0)
Collecting git+https://github.com/huggingface/transformers
  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-y8c4k41i
  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-y8c4k41i
  Resolved https://github.com/huggingface/transformers to commit 19f0b7dd02c7ea7cbf86cc87fe00667470266722
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Getting requirements to build wheel: started
  Getting requirements to build wheel: finished with status 'done'
  Preparing metadata (pyproject.toml): started
  Preparing metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: requests in /opt/miniconda/lib/python3.10/site-packages (from transformers==4.35.0.dev0) (2.31.0)
Requirement already satisfied: packaging>=20.0 in /opt/miniconda/lib/python3.10/site-packages (from transformers==4.35.0.dev0) (23.0)
Requirement already satisfied: numpy>=1.17 in /home/u131168/.local/lib/python3.10/site-packages (from transformers==4.35.0.dev0) (1.26.0)
Requirement already satisfied: tokenizers<0.15,>=0.14 in /home/u131168/.local/lib/python3.10/site-packages (from transformers==4.35.0.dev0) (0.14.0)
Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/u131168/.local/lib/python3.10/site-packages (from transformers==4.35.0.dev0) (0.16.4)
Requirement already satisfied: regex!=2019.12.17 in /home/u131168/.local/lib/python3.10/site-packages (from transformers==4.35.0.dev0) (2023.8.8)
Requirement already satisfied: safetensors>=0.3.1 in /home/u131168/.local/lib/python3.10/site-packages (from transformers==4.35.0.dev0) (0.3.3)
Requirement already satisfied: pyyaml>=5.1 in /home/u131168/.local/lib/python3.10/site-packages (from transformers==4.35.0.dev0) (6.0.1)
Requirement already satisfied: filelock in /home/u131168/.local/lib/python3.10/site-packages (from transformers==4.35.0.dev0) (3.12.4)
Requirement already satisfied: tqdm>=4.27 in /opt/miniconda/lib/python3.10/site-packages (from transformers==4.35.0.dev0) (4.65.0)
Requirement already satisfied: fsspec in /home/u131168/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.0.dev0) (2023.6.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/u131168/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.0.dev0) (4.8.0)
Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda/lib/python3.10/site-packages (from requests->transformers==4.35.0.dev0) (2023.5.7)
Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda/lib/python3.10/site-packages (from requests->transformers==4.35.0.dev0) (1.26.15)
Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda/lib/python3.10/site-packages (from requests->transformers==4.35.0.dev0) (2.0.4)
Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda/lib/python3.10/site-packages (from requests->transformers==4.35.0.dev0) (3.4)
/home/u131168/mh_one_api/model/ft_models/flan-t5-xl_peft_ft_v1/checkpoint-14800
/opt/miniconda/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.
  warn("The installed version of bitsandbytes was compiled without GPU support. "
2023-10-05 03:32:14,334 - __main__ - WARNING - Process rank: 0, device: cpu, n_gpu: 0distributed training: True, 16-bits training: False
2023-10-05 03:32:14,335 - __main__ - INFO - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=0,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/u131168/mh_one_api/model/ft_models/flan-t5-xl_peft_ft_v1/runs/Oct05_03-32-13_idc-beta-batch-pvc-node-02,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/u131168/mh_one_api/model/ft_models/flan-t5-xl_peft_ft_v1,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=2,
per_device_train_batch_size=2,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=/home/u131168/mh_one_api/model/ft_models/flan-t5-xl_peft_ft_v1/checkpoint-14800,
run_name=/home/u131168/mh_one_api/model/ft_models/flan-t5-xl_peft_ft_v1,
save_on_each_node=False,
save_safetensors=False,
save_steps=100,
save_strategy=steps,
save_total_limit=2,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
sortish_sampler=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
/home/u131168/.local/lib/python3.10/site-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=None' instead.
  warnings.warn(
Using custom data configuration default-1db59860850657b6
2023-10-05 03:32:14,665 - datasets.builder - INFO - Using custom data configuration default-1db59860850657b6
Loading Dataset Infos from /home/u131168/.local/lib/python3.10/site-packages/datasets/packaged_modules/csv
2023-10-05 03:32:14,666 - datasets.info - INFO - Loading Dataset Infos from /home/u131168/.local/lib/python3.10/site-packages/datasets/packaged_modules/csv
Generating dataset csv (/home/u131168/.cache/huggingface/datasets/csv/default-1db59860850657b6/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)
2023-10-05 03:32:14,689 - datasets.builder - INFO - Generating dataset csv (/home/u131168/.cache/huggingface/datasets/csv/default-1db59860850657b6/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)
Downloading and preparing dataset csv/default to /home/u131168/.cache/huggingface/datasets/csv/default-1db59860850657b6/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d...
2023-10-05 03:32:14,690 - datasets.builder - INFO - Downloading and preparing dataset csv/default to /home/u131168/.cache/huggingface/datasets/csv/default-1db59860850657b6/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d...
/opt/miniconda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32
-------------------starting-script
-------------------setting up-logging
----------------------detecting checkpoint
Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 6472.69it/s]
Downloading took 0.0 min
2023-10-05 03:32:14,695 - datasets.download.download_manager - INFO - Downloading took 0.0 min
Checksum Computation took 0.0 min
2023-10-05 03:32:14,695 - datasets.download.download_manager - INFO - Checksum Computation took 0.0 min
Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]Extracting data files: 100%|██████████| 1/1 [00:00<00:00,  7.16it/s]Extracting data files: 100%|██████████| 1/1 [00:00<00:00,  7.14it/s]
Generating train split
2023-10-05 03:32:14,836 - datasets.builder - INFO - Generating train split
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 10000 examples [00:00, 24875.76 examples/s]Generating train split: 20000 examples [00:00, 25946.84 examples/s]Generating train split: 30000 examples [00:01, 27183.75 examples/s]Generating train split: 40000 examples [00:01, 28576.39 examples/s]Generating train split: 50000 examples [00:01, 28392.24 examples/s]Generating train split: 60000 examples [00:02, 28667.35 examples/s]Generating train split: 70000 examples [00:02, 28973.69 examples/s]Generating train split: 80000 examples [00:02, 29212.90 examples/s]Generating train split: 90000 examples [00:03, 29247.23 examples/s]Generating train split: 100000 examples [00:03, 29418.28 examples/s]Generating train split: 110000 examples [00:03, 29047.62 examples/s]Generating train split: 120000 examples [00:04, 29655.84 examples/s]Generating train split: 130000 examples [00:04, 29381.75 examples/s]Generating train split: 140000 examples [00:04, 27570.66 examples/s]Generating train split: 150000 examples [00:05, 27847.06 examples/s]Generating train split: 160000 examples [00:05, 28035.99 examples/s]Generating train split: 170000 examples [00:05, 28344.20 examples/s]Generating train split: 180000 examples [00:06, 28461.85 examples/s]Generating train split: 190000 examples [00:06, 28806.31 examples/s]Generating train split: 200000 examples [00:07, 28920.76 examples/s]Generating train split: 210000 examples [00:07, 29388.95 examples/s]Generating train split: 220000 examples [00:07, 29239.44 examples/s]Generating train split: 230000 examples [00:08, 29244.57 examples/s]Generating train split: 240000 examples [00:08, 29208.68 examples/s]Generating train split: 250000 examples [00:08, 29055.86 examples/s]Generating train split: 260000 examples [00:09, 29153.60 examples/s]Generating train split: 270000 examples [00:09, 29322.32 examples/s]Generating train split: 280000 examples [00:10, 18408.03 examples/s]Generating train split: 290000 examples [00:10, 20585.94 examples/s]Generating train split: 300000 examples [00:11, 22715.25 examples/s]Generating train split: 310000 examples [00:11, 22062.79 examples/s]Generating train split: 320000 examples [00:11, 23590.62 examples/s]Generating train split: 330000 examples [00:12, 25008.95 examples/s]Generating train split: 340000 examples [00:12, 26223.21 examples/s]Generating train split: 350000 examples [00:12, 27369.52 examples/s]Generating train split: 360000 examples [00:13, 27383.90 examples/s]Generating train split: 370000 examples [00:13, 27430.76 examples/s]Generating train split: 380000 examples [00:14, 27611.98 examples/s]Generating train split: 390000 examples [00:14, 28073.22 examples/s]Generating train split: 395082 examples [00:14, 28488.37 examples/s]Generating train split: 395082 examples [00:14, 26481.00 examples/s]
Unable to verify splits sizes.
2023-10-05 03:32:29,761 - datasets.utils.info_utils - INFO - Unable to verify splits sizes.
Dataset csv downloaded and prepared to /home/u131168/.cache/huggingface/datasets/csv/default-1db59860850657b6/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d. Subsequent calls will reuse this data.
2023-10-05 03:32:29,776 - datasets.builder - INFO - Dataset csv downloaded and prepared to /home/u131168/.cache/huggingface/datasets/csv/default-1db59860850657b6/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d. Subsequent calls will reuse this data.
[INFO|tokenization_utils_base.py:2043] 2023-10-05 03:32:31,984 >> loading file spiece.model from cache at /home/u131168/.cache/huggingface/hub/models--google--flan-t5-xl/snapshots/8772db7a7a11f7b08e6be7d7088f7a7fd4813bc5/spiece.model
[INFO|tokenization_utils_base.py:2043] 2023-10-05 03:32:31,985 >> loading file tokenizer.json from cache at /home/u131168/.cache/huggingface/hub/models--google--flan-t5-xl/snapshots/8772db7a7a11f7b08e6be7d7088f7a7fd4813bc5/tokenizer.json
[INFO|tokenization_utils_base.py:2043] 2023-10-05 03:32:31,985 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2043] 2023-10-05 03:32:31,985 >> loading file special_tokens_map.json from cache at /home/u131168/.cache/huggingface/hub/models--google--flan-t5-xl/snapshots/8772db7a7a11f7b08e6be7d7088f7a7fd4813bc5/special_tokens_map.json
[INFO|tokenization_utils_base.py:2043] 2023-10-05 03:32:31,985 >> loading file tokenizer_config.json from cache at /home/u131168/.cache/huggingface/hub/models--google--flan-t5-xl/snapshots/8772db7a7a11f7b08e6be7d7088f7a7fd4813bc5/tokenizer_config.json
Running tokenizer on train dataset:   0%|          | 0/395082 [00:00<?, ? examples/s]Caching processed dataset at /home/u131168/.cache/huggingface/datasets/csv/default-1db59860850657b6/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-13915e0ba82a2392.arrow
2023-10-05 03:32:34,576 - datasets.arrow_dataset - INFO - Caching processed dataset at /home/u131168/.cache/huggingface/datasets/csv/default-1db59860850657b6/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-13915e0ba82a2392.arrow
Running tokenizer on train dataset:   0%|          | 1000/395082 [00:02<16:27, 399.13 examples/s]Running tokenizer on train dataset:   1%|          | 2000/395082 [00:04<16:18, 401.73 examples/s]Running tokenizer on train dataset:   1%|          | 3000/395082 [00:07<16:14, 402.47 examples/s]Running tokenizer on train dataset:   1%|          | 4000/395082 [00:09<16:05, 404.98 examples/s]Running tokenizer on train dataset:   1%|▏         | 5000/395082 [00:12<16:03, 404.82 examples/s]Running tokenizer on train dataset:   2%|▏         | 6000/395082 [00:14<15:53, 407.95 examples/s]Running tokenizer on train dataset:   2%|▏         | 7000/395082 [00:17<15:57, 405.35 examples/s]Running tokenizer on train dataset:   2%|▏         | 8000/395082 [00:19<15:47, 408.59 examples/s]Running tokenizer on train dataset:   2%|▏         | 9000/395082 [00:22<15:48, 407.19 examples/s]Running tokenizer on train dataset:   3%|▎         | 10000/395082 [00:24<15:42, 408.41 examples/s]Running tokenizer on train dataset:   3%|▎         | 11000/395082 [00:27<15:41, 408.16 examples/s]Running tokenizer on train dataset:   3%|▎         | 12000/395082 [00:29<15:16, 418.04 examples/s]Running tokenizer on train dataset:   3%|▎         | 13000/395082 [00:31<15:25, 412.62 examples/s]Running tokenizer on train dataset:   4%|▎         | 14000/395082 [00:34<15:21, 413.57 examples/s]Running tokenizer on train dataset:   4%|▍         | 15000/395082 [00:36<15:22, 412.04 examples/s]Running tokenizer on train dataset:   4%|▍         | 16000/395082 [00:39<15:22, 410.76 examples/s]Running tokenizer on train dataset:   4%|▍         | 17000/395082 [00:41<15:24, 409.06 examples/s]Running tokenizer on train dataset:   5%|▍         | 18000/395082 [00:44<15:25, 407.49 examples/s]Running tokenizer on train dataset:   5%|▍         | 19000/395082 [00:46<15:32, 403.34 examples/s]Running tokenizer on train dataset:   5%|▌         | 20000/395082 [00:49<15:33, 401.97 examples/s]Running tokenizer on train dataset:   5%|▌         | 21000/395082 [00:51<15:34, 400.30 examples/s]Running tokenizer on train dataset:   6%|▌         | 22000/395082 [00:54<15:35, 398.73 examples/s]Running tokenizer on train dataset:   6%|▌         | 23000/395082 [00:56<15:37, 396.89 examples/s]Running tokenizer on train dataset:   6%|▌         | 24000/395082 [00:59<15:22, 402.40 examples/s]Running tokenizer on train dataset:   6%|▋         | 25000/395082 [01:01<15:15, 404.29 examples/s]Running tokenizer on train dataset:   7%|▋         | 26000/395082 [01:03<15:04, 408.01 examples/s]Running tokenizer on train dataset:   7%|▋         | 27000/395082 [01:06<14:54, 411.45 examples/s]Running tokenizer on train dataset:   7%|▋         | 28000/395082 [01:08<13:39, 447.78 examples/s]Running tokenizer on train dataset:   7%|▋         | 29000/395082 [01:09<12:30, 487.99 examples/s]Running tokenizer on train dataset:   8%|▊         | 30000/395082 [01:12<13:14, 459.42 examples/s]Running tokenizer on train dataset:   8%|▊         | 31000/395082 [01:14<13:47, 439.99 examples/s]Running tokenizer on train dataset:   8%|▊         | 32000/395082 [01:17<14:08, 427.93 examples/s]Running tokenizer on train dataset:   8%|▊         | 33000/395082 [01:19<14:27, 417.43 examples/s]Running tokenizer on train dataset:   9%|▊         | 34000/395082 [01:22<14:25, 417.23 examples/s]Running tokenizer on train dataset:   9%|▉         | 35000/395082 [01:24<14:31, 413.13 examples/s]Running tokenizer on train dataset:   9%|▉         | 36000/395082 [01:27<14:35, 410.15 examples/s]Running tokenizer on train dataset:   9%|▉         | 37000/395082 [01:29<14:38, 407.42 examples/s]Running tokenizer on train dataset:  10%|▉         | 38000/395082 [01:31<14:30, 410.40 examples/s]Running tokenizer on train dataset:  10%|▉         | 39000/395082 [01:34<14:24, 411.91 examples/s]Running tokenizer on train dataset:  10%|█         | 40000/395082 [01:36<14:18, 413.57 examples/s]Running tokenizer on train dataset:  10%|█         | 41000/395082 [01:39<14:12, 415.46 examples/s]Running tokenizer on train dataset:  11%|█         | 42000/395082 [01:41<14:10, 414.94 examples/s]Running tokenizer on train dataset:  11%|█         | 43000/395082 [01:43<14:05, 416.26 examples/s]Running tokenizer on train dataset:  11%|█         | 44000/395082 [01:46<14:00, 417.86 examples/s]Running tokenizer on train dataset:  11%|█▏        | 45000/395082 [01:48<14:01, 415.88 examples/s]Running tokenizer on train dataset:  12%|█▏        | 46000/395082 [01:51<13:54, 418.09 examples/s]Running tokenizer on train dataset:  12%|█▏        | 47000/395082 [01:53<13:54, 417.35 examples/s]Running tokenizer on train dataset:  12%|█▏        | 48000/395082 [01:55<13:47, 419.61 examples/s]Running tokenizer on train dataset:  12%|█▏        | 49000/395082 [01:58<13:45, 419.27 examples/s]Running tokenizer on train dataset:  13%|█▎        | 50000/395082 [01:59<12:24, 463.80 examples/s]Running tokenizer on train dataset:  13%|█▎        | 51000/395082 [02:02<12:50, 446.51 examples/s]Running tokenizer on train dataset:  13%|█▎        | 52000/395082 [02:04<12:59, 440.13 examples/s]Running tokenizer on train dataset:  13%|█▎        | 53000/395082 [02:07<13:08, 433.68 examples/s]Running tokenizer on train dataset:  14%|█▎        | 54000/395082 [02:09<13:14, 429.13 examples/s]Running tokenizer on train dataset:  14%|█▍        | 55000/395082 [02:11<13:27, 421.21 examples/s]Running tokenizer on train dataset:  14%|█▍        | 56000/395082 [02:14<13:26, 420.68 examples/s]Running tokenizer on train dataset:  14%|█▍        | 57000/395082 [02:16<13:23, 420.97 examples/s]Running tokenizer on train dataset:  15%|█▍        | 58000/395082 [02:19<13:19, 421.45 examples/s]Running tokenizer on train dataset:  15%|█▍        | 59000/395082 [02:21<13:18, 420.67 examples/s]Running tokenizer on train dataset:  15%|█▌        | 60000/395082 [02:23<13:18, 419.76 examples/s]Running tokenizer on train dataset:  15%|█▌        | 61000/395082 [02:26<13:15, 420.13 examples/s]Running tokenizer on train dataset:  16%|█▌        | 62000/395082 [02:28<13:11, 421.01 examples/s]Running tokenizer on train dataset:  16%|█▌        | 63000/395082 [02:30<13:08, 421.31 examples/s]Running tokenizer on train dataset:  16%|█▌        | 64000/395082 [02:33<13:05, 421.39 examples/s]Running tokenizer on train dataset:  16%|█▋        | 65000/395082 [02:35<13:03, 421.38 examples/s]Running tokenizer on train dataset:  17%|█▋        | 66000/395082 [02:37<12:50, 427.13 examples/s]Running tokenizer on train dataset:  17%|█▋        | 67000/395082 [02:40<13:01, 419.73 examples/s]Running tokenizer on train dataset:  17%|█▋        | 68000/395082 [02:42<13:08, 415.03 examples/s]Running tokenizer on train dataset:  17%|█▋        | 69000/395082 [02:45<13:02, 416.77 examples/s]Running tokenizer on train dataset:  18%|█▊        | 70000/395082 [02:47<12:52, 420.86 examples/s]Running tokenizer on train dataset:  18%|█▊        | 71000/395082 [02:49<12:48, 421.86 examples/s]Running tokenizer on train dataset:  18%|█▊        | 72000/395082 [02:52<12:41, 424.14 examples/s]Running tokenizer on train dataset:  18%|█▊        | 73000/395082 [02:54<12:42, 422.55 examples/s]Running tokenizer on train dataset:  19%|█▊        | 74000/395082 [02:57<12:45, 419.34 examples/s]Running tokenizer on train dataset:  19%|█▉        | 75000/395082 [02:59<12:49, 416.07 examples/s]Running tokenizer on train dataset:  19%|█▉        | 76000/395082 [03:01<12:45, 416.64 examples/s]Running tokenizer on train dataset:  19%|█▉        | 77000/395082 [03:04<12:37, 420.02 examples/s]Running tokenizer on train dataset:  20%|█▉        | 78000/395082 [03:06<12:29, 423.02 examples/s]Running tokenizer on train dataset:  20%|█▉        | 79000/395082 [03:08<12:22, 425.62 examples/s]Running tokenizer on train dataset:  20%|██        | 80000/395082 [03:11<12:16, 427.78 examples/s]Running tokenizer on train dataset:  21%|██        | 81000/395082 [03:13<12:11, 429.56 examples/s]Running tokenizer on train dataset:  21%|██        | 82000/395082 [03:15<12:10, 428.55 examples/s]Running tokenizer on train dataset:  21%|██        | 83000/395082 [03:18<12:08, 428.54 examples/s]Running tokenizer on train dataset:  21%|██▏       | 84000/395082 [03:20<12:08, 427.24 examples/s]Running tokenizer on train dataset:  22%|██▏       | 85000/395082 [03:22<12:05, 427.28 examples/s]Running tokenizer on train dataset:  22%|██▏       | 86000/395082 [03:25<12:01, 428.48 examples/s]Running tokenizer on train dataset:  22%|██▏       | 87000/395082 [03:27<12:03, 425.78 examples/s]Running tokenizer on train dataset:  22%|██▏       | 88000/395082 [03:30<12:09, 420.81 examples/s]Running tokenizer on train dataset:  23%|██▎       | 89000/395082 [03:32<12:15, 416.16 examples/s]Running tokenizer on train dataset:  23%|██▎       | 90000/395082 [03:34<12:06, 419.72 examples/s]Running tokenizer on train dataset:  23%|██▎       | 91000/395082 [03:37<12:07, 417.81 examples/s]Running tokenizer on train dataset:  23%|██▎       | 92000/395082 [03:39<11:58, 421.62 examples/s]Running tokenizer on train dataset:  24%|██▎       | 93000/395082 [03:41<11:52, 424.24 examples/s]Running tokenizer on train dataset:  24%|██▍       | 94000/395082 [03:44<11:51, 422.92 examples/s]Running tokenizer on train dataset:  24%|██▍       | 95000/395082 [03:45<10:19, 484.76 examples/s]Running tokenizer on train dataset:  24%|██▍       | 96000/395082 [03:46<09:06, 546.93 examples/s]Running tokenizer on train dataset:  25%|██▍       | 97000/395082 [03:49<09:39, 514.03 examples/s]Running tokenizer on train dataset:  25%|██▍       | 98000/395082 [03:51<10:11, 485.52 examples/s]Running tokenizer on train dataset:  25%|██▌       | 99000/395082 [03:53<10:45, 458.91 examples/s]Running tokenizer on train dataset:  25%|██▌       | 100000/395082 [03:56<11:04, 444.21 examples/s]Running tokenizer on train dataset:  26%|██▌       | 101000/395082 [03:58<11:16, 434.91 examples/s]Running tokenizer on train dataset:  26%|██▌       | 102000/395082 [04:01<11:23, 429.08 examples/s]Running tokenizer on train dataset:  26%|██▌       | 103000/395082 [04:03<11:26, 425.46 examples/s]Running tokenizer on train dataset:  26%|██▋       | 104000/395082 [04:05<11:28, 422.53 examples/s]Running tokenizer on train dataset:  27%|██▋       | 105000/395082 [04:08<11:30, 419.92 examples/s]Running tokenizer on train dataset:  27%|██▋       | 106000/395082 [04:10<11:33, 416.67 examples/s]Running tokenizer on train dataset:  27%|██▋       | 107000/395082 [04:13<11:32, 416.06 examples/s]Running tokenizer on train dataset:  27%|██▋       | 108000/395082 [04:15<11:30, 415.86 examples/s]Running tokenizer on train dataset:  28%|██▊       | 109000/395082 [04:18<11:27, 416.33 examples/s]Running tokenizer on train dataset:  28%|██▊       | 110000/395082 [04:20<11:25, 416.10 examples/s]Running tokenizer on train dataset:  28%|██▊       | 111000/395082 [04:22<11:22, 416.40 examples/s]Running tokenizer on train dataset:  28%|██▊       | 112000/395082 [04:25<11:19, 416.85 examples/s]Running tokenizer on train dataset:  29%|██▊       | 113000/395082 [04:27<11:22, 413.23 examples/s]Running tokenizer on train dataset:  29%|██▉       | 114000/395082 [04:30<11:33, 405.35 examples/s]Running tokenizer on train dataset:  29%|██▉       | 115000/395082 [04:32<11:37, 401.53 examples/s]Running tokenizer on train dataset:  29%|██▉       | 116000/395082 [04:35<11:40, 398.57 examples/s]Running tokenizer on train dataset:  30%|██▉       | 117000/395082 [04:37<11:40, 396.94 examples/s]Running tokenizer on train dataset:  30%|██▉       | 118000/395082 [04:40<11:38, 396.82 examples/s]Running tokenizer on train dataset:  30%|███       | 119000/395082 [04:42<11:30, 399.95 examples/s]Running tokenizer on train dataset:  30%|███       | 120000/395082 [04:45<11:24, 402.11 examples/s]Running tokenizer on train dataset:  31%|███       | 121000/395082 [04:47<11:19, 403.21 examples/s]Running tokenizer on train dataset:  31%|███       | 122000/395082 [04:50<11:19, 401.74 examples/s]Running tokenizer on train dataset:  31%|███       | 123000/395082 [04:52<11:11, 405.21 examples/s]Running tokenizer on train dataset:  31%|███▏      | 124000/395082 [04:55<11:06, 406.50 examples/s]Running tokenizer on train dataset:  32%|███▏      | 125000/395082 [04:57<11:04, 406.31 examples/s]Running tokenizer on train dataset:  32%|███▏      | 126000/395082 [05:00<10:59, 407.79 examples/s]Running tokenizer on train dataset:  32%|███▏      | 127000/395082 [05:02<10:58, 406.98 examples/s]Running tokenizer on train dataset:  32%|███▏      | 128000/395082 [05:05<11:06, 401.00 examples/s]Running tokenizer on train dataset:  33%|███▎      | 129000/395082 [05:07<10:59, 403.23 examples/s]Running tokenizer on train dataset:  33%|███▎      | 130000/395082 [05:10<10:54, 404.72 examples/s]Running tokenizer on train dataset:  33%|███▎      | 131000/395082 [05:12<10:51, 405.37 examples/s]Running tokenizer on train dataset:  33%|███▎      | 132000/395082 [05:14<10:46, 406.90 examples/s]Running tokenizer on train dataset:  34%|███▎      | 133000/395082 [05:17<10:52, 401.77 examples/s]Running tokenizer on train dataset:  34%|███▍      | 134000/395082 [05:20<10:55, 398.31 examples/s]Running tokenizer on train dataset:  34%|███▍      | 135000/395082 [05:22<10:58, 395.12 examples/s]Running tokenizer on train dataset:  34%|███▍      | 136000/395082 [05:25<10:53, 396.63 examples/s]Running tokenizer on train dataset:  35%|███▍      | 137000/395082 [05:27<10:49, 397.64 examples/s]Running tokenizer on train dataset:  35%|███▍      | 138000/395082 [05:30<10:45, 398.16 examples/s]Running tokenizer on train dataset:  35%|███▌      | 139000/395082 [05:32<10:46, 396.25 examples/s]Running tokenizer on train dataset:  35%|███▌      | 140000/395082 [05:35<10:45, 395.15 examples/s]Running tokenizer on train dataset:  36%|███▌      | 141000/395082 [05:37<10:43, 394.68 examples/s]Running tokenizer on train dataset:  36%|███▌      | 142000/395082 [05:40<10:43, 393.30 examples/s]Running tokenizer on train dataset:  36%|███▌      | 143000/395082 [05:42<10:43, 392.00 examples/s]Running tokenizer on train dataset:  36%|███▋      | 144000/395082 [05:45<10:35, 395.35 examples/s]Running tokenizer on train dataset:  37%|███▋      | 145000/395082 [05:47<10:31, 396.30 examples/s]Running tokenizer on train dataset:  37%|███▋      | 146000/395082 [05:50<10:31, 394.62 examples/s]Running tokenizer on train dataset:  37%|███▋      | 147000/395082 [05:53<10:33, 391.83 examples/s]Running tokenizer on train dataset:  37%|███▋      | 148000/395082 [05:55<10:26, 394.12 examples/s]Running tokenizer on train dataset:  38%|███▊      | 149000/395082 [05:58<10:31, 389.38 examples/s]Running tokenizer on train dataset:  38%|███▊      | 150000/395082 [06:00<10:25, 391.95 examples/s]Running tokenizer on train dataset:  38%|███▊      | 151000/395082 [06:03<10:22, 391.97 examples/s]Running tokenizer on train dataset:  38%|███▊      | 152000/395082 [06:05<10:19, 392.24 examples/s]Running tokenizer on train dataset:  39%|███▊      | 153000/395082 [06:08<10:13, 394.65 examples/s]Running tokenizer on train dataset:  39%|███▉      | 154000/395082 [06:10<10:08, 396.37 examples/s]Running tokenizer on train dataset:  39%|███▉      | 155000/395082 [06:13<10:02, 398.17 examples/s]Running tokenizer on train dataset:  39%|███▉      | 156000/395082 [06:15<10:00, 398.20 examples/s]Running tokenizer on train dataset:  40%|███▉      | 157000/395082 [06:18<09:56, 399.05 examples/s]Running tokenizer on train dataset:  40%|███▉      | 158000/395082 [06:20<09:54, 399.08 examples/s]Running tokenizer on train dataset:  40%|████      | 159000/395082 [06:23<09:58, 394.77 examples/s]Running tokenizer on train dataset:  40%|████      | 160000/395082 [06:26<10:03, 389.79 examples/s]Running tokenizer on train dataset:  41%|████      | 161000/395082 [06:28<10:05, 386.71 examples/s]Running tokenizer on train dataset:  41%|████      | 162000/395082 [06:31<09:59, 388.50 examples/s]Running tokenizer on train dataset:  41%|████▏     | 163000/395082 [06:33<09:59, 386.86 examples/s]Running tokenizer on train dataset:  42%|████▏     | 164000/395082 [06:36<10:02, 383.79 examples/s]Running tokenizer on train dataset:  42%|████▏     | 165000/395082 [06:39<09:56, 385.83 examples/s]Running tokenizer on train dataset:  42%|████▏     | 166000/395082 [06:41<09:53, 386.11 examples/s]Running tokenizer on train dataset:  42%|████▏     | 167000/395082 [06:44<09:47, 388.33 examples/s]Running tokenizer on train dataset:  43%|████▎     | 168000/395082 [06:46<09:40, 391.09 examples/s]Running tokenizer on train dataset:  43%|████▎     | 169000/395082 [06:49<09:40, 389.78 examples/s]Running tokenizer on train dataset:  43%|████▎     | 170000/395082 [06:51<09:41, 387.24 examples/s]Running tokenizer on train dataset:  43%|████▎     | 171000/395082 [06:54<09:41, 385.65 examples/s]Running tokenizer on train dataset:  44%|████▎     | 172000/395082 [06:57<09:36, 386.63 examples/s]Running tokenizer on train dataset:  44%|████▍     | 173000/395082 [06:59<09:26, 391.79 examples/s]Running tokenizer on train dataset:  44%|████▍     | 174000/395082 [07:02<09:24, 391.39 examples/s]Running tokenizer on train dataset:  44%|████▍     | 175000/395082 [07:04<09:21, 392.22 examples/s]Running tokenizer on train dataset:  45%|████▍     | 176000/395082 [07:07<09:17, 393.11 examples/s]Running tokenizer on train dataset:  45%|████▍     | 177000/395082 [07:09<09:13, 393.71 examples/s]Running tokenizer on train dataset:  45%|████▌     | 178000/395082 [07:12<09:12, 392.63 examples/s]Running tokenizer on train dataset:  45%|████▌     | 179000/395082 [07:14<09:11, 391.98 examples/s]Running tokenizer on train dataset:  46%|████▌     | 180000/395082 [07:17<09:08, 392.20 examples/s]Running tokenizer on train dataset:  46%|████▌     | 181000/395082 [07:19<09:03, 393.78 examples/s]Running tokenizer on train dataset:  46%|████▌     | 182000/395082 [07:22<09:02, 392.94 examples/s]Running tokenizer on train dataset:  46%|████▋     | 183000/395082 [07:24<09:00, 392.33 examples/s]Running tokenizer on train dataset:  47%|████▋     | 184000/395082 [07:27<09:02, 388.85 examples/s]Running tokenizer on train dataset:  47%|████▋     | 185000/395082 [07:30<09:08, 383.04 examples/s]Running tokenizer on train dataset:  47%|████▋     | 186000/395082 [07:32<09:00, 386.98 examples/s]Running tokenizer on train dataset:  47%|████▋     | 187000/395082 [07:35<08:55, 388.31 examples/s]Running tokenizer on train dataset:  48%|████▊     | 188000/395082 [07:37<08:54, 387.44 examples/s]Running tokenizer on train dataset:  48%|████▊     | 189000/395082 [07:40<08:47, 390.54 examples/s]Running tokenizer on train dataset:  48%|████▊     | 190000/395082 [07:43<08:45, 390.08 examples/s]Running tokenizer on train dataset:  48%|████▊     | 191000/395082 [07:45<08:46, 387.98 examples/s]Running tokenizer on train dataset:  49%|████▊     | 192000/395082 [07:48<08:42, 388.65 examples/s]Running tokenizer on train dataset:  49%|████▉     | 193000/395082 [07:50<08:44, 385.21 examples/s]Running tokenizer on train dataset:  49%|████▉     | 194000/395082 [07:53<08:43, 383.88 examples/s]Running tokenizer on train dataset:  49%|████▉     | 195000/395082 [07:56<08:34, 388.51 examples/s]Running tokenizer on train dataset:  50%|████▉     | 196000/395082 [07:58<08:30, 389.78 examples/s]Running tokenizer on train dataset:  50%|████▉     | 197000/395082 [08:00<07:42, 428.30 examples/s]Running tokenizer on train dataset:  50%|█████     | 198000/395082 [08:01<06:45, 486.18 examples/s]Running tokenizer on train dataset:  50%|█████     | 199000/395082 [08:04<07:12, 453.49 examples/s]Running tokenizer on train dataset:  51%|█████     | 200000/395082 [08:06<07:30, 433.15 examples/s]Running tokenizer on train dataset:  51%|█████     | 201000/395082 [08:09<07:47, 415.40 examples/s]Running tokenizer on train dataset:  51%|█████     | 202000/395082 [08:12<07:54, 406.79 examples/s]Running tokenizer on train dataset:  51%|█████▏    | 203000/395082 [08:14<08:01, 398.60 examples/s]Running tokenizer on train dataset:  52%|█████▏    | 204000/395082 [08:17<08:04, 394.42 examples/s]Running tokenizer on train dataset:  52%|█████▏    | 205000/395082 [08:19<07:24, 427.15 examples/s]Running tokenizer on train dataset:  52%|█████▏    | 206000/395082 [08:20<06:26, 488.63 examples/s]Running tokenizer on train dataset:  52%|█████▏    | 207000/395082 [08:21<05:45, 543.70 examples/s]Running tokenizer on train dataset:  53%|█████▎    | 208000/395082 [08:23<05:18, 588.28 examples/s]Running tokenizer on train dataset:  53%|█████▎    | 209000/395082 [08:25<06:00, 516.70 examples/s]Running tokenizer on train dataset:  53%|█████▎    | 210000/395082 [08:27<05:37, 547.67 examples/s]Running tokenizer on train dataset:  53%|█████▎    | 211000/395082 [08:28<05:18, 578.34 examples/s]Running tokenizer on train dataset:  54%|█████▎    | 212000/395082 [08:30<04:59, 611.80 examples/s]Running tokenizer on train dataset:  54%|█████▍    | 213000/395082 [08:31<04:42, 643.58 examples/s]Running tokenizer on train dataset:  54%|█████▍    | 214000/395082 [08:34<05:26, 554.30 examples/s]Running tokenizer on train dataset:  54%|█████▍    | 215000/395082 [08:36<06:10, 485.49 examples/s]Running tokenizer on train dataset:  55%|█████▍    | 216000/395082 [08:39<06:40, 447.03 examples/s]Running tokenizer on train dataset:  55%|█████▍    | 217000/395082 [08:41<06:59, 424.15 examples/s]Running tokenizer on train dataset:  55%|█████▌    | 218000/395082 [08:44<07:08, 413.41 examples/s]Running tokenizer on train dataset:  55%|█████▌    | 219000/395082 [08:47<07:13, 406.55 examples/s]Running tokenizer on train dataset:  56%|█████▌    | 220000/395082 [08:48<06:26, 453.10 examples/s]Running tokenizer on train dataset:  56%|█████▌    | 221000/395082 [08:50<05:40, 511.76 examples/s]Running tokenizer on train dataset:  56%|█████▌    | 222000/395082 [08:52<06:11, 466.18 examples/s]Running tokenizer on train dataset:  56%|█████▋    | 223000/395082 [08:55<06:34, 435.88 examples/s]Running tokenizer on train dataset:  57%|█████▋    | 224000/395082 [08:57<06:50, 416.29 examples/s]Running tokenizer on train dataset:  57%|█████▋    | 225000/395082 [09:00<07:01, 403.29 examples/s]Running tokenizer on train dataset:  57%|█████▋    | 226000/395082 [09:03<07:10, 392.73 examples/s]Running tokenizer on train dataset:  57%|█████▋    | 227000/395082 [09:05<07:14, 386.84 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 228000/395082 [09:08<07:14, 384.42 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 229000/395082 [09:11<07:09, 386.54 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 230000/395082 [09:13<07:10, 383.37 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 231000/395082 [09:16<07:12, 379.81 examples/s]Running tokenizer on train dataset:  59%|█████▊    | 232000/395082 [09:19<07:09, 379.92 examples/s]Running tokenizer on train dataset:  59%|█████▉    | 233000/395082 [09:21<07:05, 381.31 examples/s]Running tokenizer on train dataset:  59%|█████▉    | 234000/395082 [09:24<07:01, 382.53 examples/s]Running tokenizer on train dataset:  59%|█████▉    | 235000/395082 [09:26<06:59, 381.54 examples/s]Running tokenizer on train dataset:  60%|█████▉    | 236000/395082 [09:29<06:54, 383.98 examples/s]Running tokenizer on train dataset:  60%|█████▉    | 237000/395082 [09:30<05:53, 446.62 examples/s]Running tokenizer on train dataset:  60%|██████    | 238000/395082 [09:32<05:10, 505.95 examples/s]Running tokenizer on train dataset:  60%|██████    | 239000/395082 [09:33<04:51, 535.92 examples/s]Running tokenizer on train dataset:  61%|██████    | 240000/395082 [09:36<05:26, 475.08 examples/s]Running tokenizer on train dataset:  61%|██████    | 241000/395082 [09:39<05:41, 451.29 examples/s]Running tokenizer on train dataset:  61%|██████▏   | 242000/395082 [09:40<05:00, 509.47 examples/s]Running tokenizer on train dataset:  62%|██████▏   | 243000/395082 [09:41<04:31, 560.36 examples/s]Running tokenizer on train dataset:  62%|██████▏   | 244000/395082 [09:44<04:51, 518.26 examples/s]Running tokenizer on train dataset:  62%|██████▏   | 245000/395082 [09:46<05:17, 472.25 examples/s]Running tokenizer on train dataset:  62%|██████▏   | 246000/395082 [09:49<05:35, 444.52 examples/s]Running tokenizer on train dataset:  63%|██████▎   | 247000/395082 [09:51<05:44, 429.44 examples/s]Running tokenizer on train dataset:  63%|██████▎   | 248000/395082 [09:54<05:51, 418.26 examples/s]Running tokenizer on train dataset:  63%|██████▎   | 249000/395082 [09:56<05:55, 411.09 examples/s]Running tokenizer on train dataset:  63%|██████▎   | 250000/395082 [09:59<06:00, 402.63 examples/s]Running tokenizer on train dataset:  64%|██████▎   | 251000/395082 [10:01<05:42, 421.05 examples/s]Running tokenizer on train dataset:  64%|██████▍   | 252000/395082 [10:04<05:51, 407.01 examples/s]Running tokenizer on train dataset:  64%|██████▍   | 253000/395082 [10:06<05:56, 398.13 examples/s]Running tokenizer on train dataset:  64%|██████▍   | 254000/395082 [10:09<05:58, 393.25 examples/s]Running tokenizer on train dataset:  65%|██████▍   | 255000/395082 [10:11<05:58, 390.24 examples/s]Running tokenizer on train dataset:  65%|██████▍   | 256000/395082 [10:14<05:59, 386.88 examples/s]Running tokenizer on train dataset:  65%|██████▌   | 257000/395082 [10:17<05:55, 388.87 examples/s]Running tokenizer on train dataset:  65%|██████▌   | 258000/395082 [10:19<05:50, 390.97 examples/s]Running tokenizer on train dataset:  66%|██████▌   | 259000/395082 [10:22<05:48, 390.22 examples/s]Running tokenizer on train dataset:  66%|██████▌   | 260000/395082 [10:24<05:44, 391.70 examples/s]Running tokenizer on train dataset:  66%|██████▌   | 261000/395082 [10:27<05:40, 393.30 examples/s]Running tokenizer on train dataset:  66%|██████▋   | 262000/395082 [10:29<05:41, 389.96 examples/s]Running tokenizer on train dataset:  67%|██████▋   | 263000/395082 [10:32<05:41, 386.74 examples/s]Running tokenizer on train dataset:  67%|██████▋   | 264000/395082 [10:35<05:36, 389.60 examples/s]Running tokenizer on train dataset:  67%|██████▋   | 265000/395082 [10:37<05:36, 387.01 examples/s]Running tokenizer on train dataset:  67%|██████▋   | 266000/395082 [10:40<05:34, 385.55 examples/s]Running tokenizer on train dataset:  68%|██████▊   | 267000/395082 [10:42<05:34, 383.41 examples/s]Running tokenizer on train dataset:  68%|██████▊   | 268000/395082 [10:45<05:31, 382.88 examples/s]Running tokenizer on train dataset:  68%|██████▊   | 269000/395082 [10:48<05:26, 386.00 examples/s]Running tokenizer on train dataset:  68%|██████▊   | 270000/395082 [10:50<05:22, 388.23 examples/s]Running tokenizer on train dataset:  69%|██████▊   | 271000/395082 [10:53<05:19, 388.50 examples/s]Running tokenizer on train dataset:  69%|██████▉   | 272000/395082 [10:55<05:15, 389.61 examples/s]Running tokenizer on train dataset:  69%|██████▉   | 273000/395082 [10:57<04:32, 447.20 examples/s]Running tokenizer on train dataset:  69%|██████▉   | 274000/395082 [10:58<04:00, 503.57 examples/s]Running tokenizer on train dataset:  70%|██████▉   | 275000/395082 [11:00<04:00, 499.32 examples/s]Running tokenizer on train dataset:  70%|██████▉   | 276000/395082 [11:03<04:17, 462.69 examples/s]Running tokenizer on train dataset:  70%|███████   | 277000/395082 [11:06<04:42, 418.12 examples/s]Running tokenizer on train dataset:  70%|███████   | 278000/395082 [11:08<04:48, 405.64 examples/s]Running tokenizer on train dataset:  71%|███████   | 279000/395082 [11:11<04:52, 396.78 examples/s]Running tokenizer on train dataset:  71%|███████   | 280000/395082 [11:14<04:55, 389.97 examples/s]Running tokenizer on train dataset:  71%|███████   | 281000/395082 [11:16<04:55, 385.44 examples/s]Running tokenizer on train dataset:  71%|███████▏  | 282000/395082 [11:19<04:54, 384.43 examples/s]Running tokenizer on train dataset:  72%|███████▏  | 283000/395082 [11:21<04:51, 384.75 examples/s]Running tokenizer on train dataset:  72%|███████▏  | 284000/395082 [11:24<04:47, 386.63 examples/s]Running tokenizer on train dataset:  72%|███████▏  | 285000/395082 [11:27<04:47, 382.61 examples/s]Running tokenizer on train dataset:  72%|███████▏  | 286000/395082 [11:29<04:45, 381.60 examples/s]Running tokenizer on train dataset:  73%|███████▎  | 287000/395082 [11:32<04:40, 384.89 examples/s]Running tokenizer on train dataset:  73%|███████▎  | 288000/395082 [11:34<04:35, 388.44 examples/s]Running tokenizer on train dataset:  73%|███████▎  | 289000/395082 [11:37<04:30, 391.96 examples/s]Running tokenizer on train dataset:  73%|███████▎  | 290000/395082 [11:39<04:27, 393.55 examples/s]Running tokenizer on train dataset:  74%|███████▎  | 291000/395082 [11:42<04:23, 395.25 examples/s]Running tokenizer on train dataset:  74%|███████▍  | 292000/395082 [11:44<04:21, 394.87 examples/s]Running tokenizer on train dataset:  74%|███████▍  | 293000/395082 [11:47<04:17, 396.22 examples/s]Running tokenizer on train dataset:  74%|███████▍  | 294000/395082 [11:49<04:14, 397.50 examples/s]Running tokenizer on train dataset:  75%|███████▍  | 295000/395082 [11:52<04:14, 393.36 examples/s]Running tokenizer on train dataset:  75%|███████▍  | 296000/395082 [11:55<04:13, 391.29 examples/s]Running tokenizer on train dataset:  75%|███████▌  | 297000/395082 [11:57<04:10, 392.23 examples/s]Running tokenizer on train dataset:  75%|███████▌  | 298000/395082 [11:59<03:57, 408.85 examples/s]Running tokenizer on train dataset:  76%|███████▌  | 299000/395082 [12:01<03:23, 471.85 examples/s]Running tokenizer on train dataset:  76%|███████▌  | 300000/395082 [12:02<03:00, 527.67 examples/s]Running tokenizer on train dataset:  76%|███████▌  | 301000/395082 [12:04<02:43, 574.44 examples/s]Running tokenizer on train dataset:  76%|███████▋  | 302000/395082 [12:05<02:32, 608.46 examples/s]Running tokenizer on train dataset:  77%|███████▋  | 303000/395082 [12:06<02:23, 641.31 examples/s]Running tokenizer on train dataset:  77%|███████▋  | 304000/395082 [12:08<02:26, 620.12 examples/s]Running tokenizer on train dataset:  77%|███████▋  | 305000/395082 [12:11<02:51, 525.20 examples/s]Running tokenizer on train dataset:  77%|███████▋  | 306000/395082 [12:13<03:07, 476.13 examples/s]Running tokenizer on train dataset:  78%|███████▊  | 307000/395082 [12:15<02:55, 503.19 examples/s]Running tokenizer on train dataset:  78%|███████▊  | 308000/395082 [12:16<02:37, 552.56 examples/s]Running tokenizer on train dataset:  78%|███████▊  | 309000/395082 [12:18<02:24, 596.90 examples/s]Running tokenizer on train dataset:  78%|███████▊  | 310000/395082 [12:19<02:15, 626.03 examples/s]Running tokenizer on train dataset:  79%|███████▊  | 311000/395082 [12:20<02:08, 653.89 examples/s]Running tokenizer on train dataset:  79%|███████▉  | 312000/395082 [12:22<02:03, 672.18 examples/s]Running tokenizer on train dataset:  79%|███████▉  | 313000/395082 [12:23<01:59, 686.09 examples/s]Running tokenizer on train dataset:  79%|███████▉  | 314000/395082 [12:25<01:59, 677.42 examples/s]Running tokenizer on train dataset:  80%|███████▉  | 315000/395082 [12:27<02:04, 640.67 examples/s]Running tokenizer on train dataset:  80%|███████▉  | 316000/395082 [12:28<02:04, 637.63 examples/s]Running tokenizer on train dataset:  80%|████████  | 317000/395082 [12:30<02:13, 584.06 examples/s]Running tokenizer on train dataset:  80%|████████  | 318000/395082 [12:33<02:34, 498.15 examples/s]Running tokenizer on train dataset:  81%|████████  | 319000/395082 [12:35<02:46, 456.32 examples/s]Running tokenizer on train dataset:  81%|████████  | 320000/395082 [12:38<02:54, 429.62 examples/s]Running tokenizer on train dataset:  81%|████████  | 321000/395082 [12:41<02:58, 415.89 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 322000/395082 [12:43<03:02, 400.99 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 323000/395082 [12:46<03:05, 387.72 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 324000/395082 [12:48<02:55, 405.55 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 325000/395082 [12:50<02:30, 466.60 examples/s]Running tokenizer on train dataset:  83%|████████▎ | 326000/395082 [12:51<02:12, 519.74 examples/s]Running tokenizer on train dataset:  83%|████████▎ | 327000/395082 [12:53<01:59, 567.42 examples/s]Running tokenizer on train dataset:  83%|████████▎ | 328000/395082 [12:55<02:08, 523.63 examples/s]Running tokenizer on train dataset:  83%|████████▎ | 329000/395082 [12:57<02:02, 541.27 examples/s]Running tokenizer on train dataset:  84%|████████▎ | 330000/395082 [12:58<01:50, 589.97 examples/s]Running tokenizer on train dataset:  84%|████████▍ | 331000/395082 [13:00<01:48, 592.12 examples/s]Running tokenizer on train dataset:  84%|████████▍ | 332000/395082 [13:02<02:05, 504.29 examples/s]Running tokenizer on train dataset:  84%|████████▍ | 333000/395082 [13:05<02:14, 460.78 examples/s]Running tokenizer on train dataset:  85%|████████▍ | 334000/395082 [13:06<01:58, 517.24 examples/s]Running tokenizer on train dataset:  85%|████████▍ | 335000/395082 [13:08<02:00, 499.47 examples/s]Running tokenizer on train dataset:  85%|████████▌ | 336000/395082 [13:11<02:09, 456.89 examples/s]Running tokenizer on train dataset:  85%|████████▌ | 337000/395082 [13:14<02:14, 431.58 examples/s]Running tokenizer on train dataset:  86%|████████▌ | 338000/395082 [13:16<02:17, 414.87 examples/s]Running tokenizer on train dataset:  86%|████████▌ | 339000/395082 [13:18<01:59, 468.90 examples/s]Running tokenizer on train dataset:  86%|████████▌ | 340000/395082 [13:19<01:45, 520.90 examples/s]Running tokenizer on train dataset:  86%|████████▋ | 341000/395082 [13:21<01:35, 568.18 examples/s]Running tokenizer on train dataset:  87%|████████▋ | 342000/395082 [13:22<01:28, 602.59 examples/s]Running tokenizer on train dataset:  87%|████████▋ | 343000/395082 [13:24<01:28, 585.77 examples/s]Running tokenizer on train dataset:  87%|████████▋ | 344000/395082 [13:26<01:42, 500.03 examples/s]Running tokenizer on train dataset:  87%|████████▋ | 345000/395082 [13:29<01:50, 452.35 examples/s]Running tokenizer on train dataset:  88%|████████▊ | 346000/395082 [13:32<01:54, 427.06 examples/s]Running tokenizer on train dataset:  88%|████████▊ | 347000/395082 [13:34<01:57, 409.58 examples/s]Running tokenizer on train dataset:  88%|████████▊ | 348000/395082 [13:37<01:57, 399.28 examples/s]Running tokenizer on train dataset:  88%|████████▊ | 349000/395082 [13:40<01:57, 392.01 examples/s]Running tokenizer on train dataset:  89%|████████▊ | 350000/395082 [13:42<01:56, 386.46 examples/s]Running tokenizer on train dataset:  89%|████████▉ | 351000/395082 [13:45<01:55, 381.28 examples/s]Running tokenizer on train dataset:  89%|████████▉ | 352000/395082 [13:48<01:55, 374.30 examples/s]Running tokenizer on train dataset:  89%|████████▉ | 353000/395082 [13:51<01:52, 374.16 examples/s]Running tokenizer on train dataset:  90%|████████▉ | 354000/395082 [13:53<01:49, 373.92 examples/s]Running tokenizer on train dataset:  90%|████████▉ | 355000/395082 [13:56<01:47, 373.77 examples/s]Running tokenizer on train dataset:  90%|█████████ | 356000/395082 [13:59<01:43, 376.04 examples/s]Running tokenizer on train dataset:  90%|█████████ | 357000/395082 [14:01<01:41, 375.74 examples/s]Running tokenizer on train dataset:  91%|█████████ | 358000/395082 [14:04<01:39, 372.91 examples/s]Running tokenizer on train dataset:  91%|█████████ | 359000/395082 [14:07<01:37, 371.75 examples/s]Running tokenizer on train dataset:  91%|█████████ | 360000/395082 [14:09<01:34, 371.41 examples/s]Running tokenizer on train dataset:  91%|█████████▏| 361000/395082 [14:12<01:32, 370.35 examples/s]Running tokenizer on train dataset:  92%|█████████▏| 362000/395082 [14:15<01:29, 369.15 examples/s]Running tokenizer on train dataset:  92%|█████████▏| 363000/395082 [14:18<01:28, 364.35 examples/s]Running tokenizer on train dataset:  92%|█████████▏| 364000/395082 [14:21<01:26, 359.16 examples/s]Running tokenizer on train dataset:  92%|█████████▏| 365000/395082 [14:23<01:24, 356.16 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 366000/395082 [14:26<01:21, 357.35 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 367000/395082 [14:29<01:18, 356.49 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 368000/395082 [14:32<01:13, 366.01 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 369000/395082 [14:34<01:12, 361.52 examples/s]Running tokenizer on train dataset:  94%|█████████▎| 370000/395082 [14:37<01:10, 357.49 examples/s]Running tokenizer on train dataset:  94%|█████████▍| 371000/395082 [14:40<01:07, 356.58 examples/s]Running tokenizer on train dataset:  94%|█████████▍| 372000/395082 [14:43<01:04, 355.88 examples/s]Running tokenizer on train dataset:  94%|█████████▍| 373000/395082 [14:46<01:01, 358.34 examples/s]Running tokenizer on train dataset:  95%|█████████▍| 374000/395082 [14:48<00:57, 367.42 examples/s]Running tokenizer on train dataset:  95%|█████████▍| 375000/395082 [14:50<00:50, 399.04 examples/s]Running tokenizer on train dataset:  95%|█████████▌| 376000/395082 [14:53<00:49, 385.27 examples/s]Running tokenizer on train dataset:  95%|█████████▌| 377000/395082 [14:56<00:47, 379.01 examples/s]Running tokenizer on train dataset:  96%|█████████▌| 378000/395082 [14:59<00:45, 374.15 examples/s]Running tokenizer on train dataset:  96%|█████████▌| 379000/395082 [15:00<00:38, 422.85 examples/s]Running tokenizer on train dataset:  96%|█████████▌| 380000/395082 [15:02<00:31, 479.54 examples/s]Running tokenizer on train dataset:  96%|█████████▋| 381000/395082 [15:03<00:26, 529.40 examples/s]Running tokenizer on train dataset:  97%|█████████▋| 382000/395082 [15:05<00:24, 540.10 examples/s]Running tokenizer on train dataset:  97%|█████████▋| 383000/395082 [15:08<00:25, 471.16 examples/s]Running tokenizer on train dataset:  97%|█████████▋| 384000/395082 [15:10<00:26, 425.35 examples/s]Running tokenizer on train dataset:  97%|█████████▋| 385000/395082 [15:13<00:25, 397.86 examples/s]Running tokenizer on train dataset:  98%|█████████▊| 386000/395082 [15:16<00:23, 381.16 examples/s]Running tokenizer on train dataset:  98%|█████████▊| 387000/395082 [15:18<00:20, 396.73 examples/s]Running tokenizer on train dataset:  98%|█████████▊| 388000/395082 [15:21<00:18, 384.62 examples/s]Running tokenizer on train dataset:  98%|█████████▊| 389000/395082 [15:24<00:16, 376.84 examples/s]Running tokenizer on train dataset:  99%|█████████▊| 390000/395082 [15:26<00:12, 400.38 examples/s]Running tokenizer on train dataset:  99%|█████████▉| 391000/395082 [15:28<00:09, 440.64 examples/s]Running tokenizer on train dataset:  99%|█████████▉| 392000/395082 [15:31<00:07, 407.64 examples/s]Running tokenizer on train dataset:  99%|█████████▉| 393000/395082 [15:33<00:05, 405.03 examples/s]Running tokenizer on train dataset: 100%|█████████▉| 394000/395082 [15:35<00:02, 460.10 examples/s]Running tokenizer on train dataset: 100%|█████████▉| 395000/395082 [15:36<00:00, 510.71 examples/s]Running tokenizer on train dataset: 100%|██████████| 395082/395082 [15:36<00:00, 514.54 examples/s]Running tokenizer on train dataset: 100%|██████████| 395082/395082 [15:38<00:00, 420.83 examples/s]
[INFO|configuration_utils.py:715] 2023-10-05 03:48:11,772 >> loading configuration file config.json from cache at /home/u131168/.cache/huggingface/hub/models--google--flan-t5-xl/snapshots/8772db7a7a11f7b08e6be7d7088f7a7fd4813bc5/config.json
[INFO|configuration_utils.py:775] 2023-10-05 03:48:11,780 >> Model config T5Config {
  "_name_or_path": "google/flan-t5-xl",
  "architectures": [
    "T5ForConditionalGeneration"
  ],
  "classifier_dropout": 0.0,
  "d_ff": 5120,
  "d_kv": 64,
  "d_model": 2048,
  "decoder_start_token_id": 0,
  "dense_act_fn": "gelu_new",
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "gated-gelu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "t5",
  "n_positions": 512,
  "num_decoder_layers": 24,
  "num_heads": 32,
  "num_layers": 24,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "task_specific_params": {
    "summarization": {
      "early_stopping": true,
      "length_penalty": 2.0,
      "max_length": 200,
      "min_length": 30,
      "no_repeat_ngram_size": 3,
      "num_beams": 4,
      "prefix": "summarize: "
    },
    "translation_en_to_de": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to German: "
    },
    "translation_en_to_fr": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to French: "
    },
    "translation_en_to_ro": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to Romanian: "
    }
  },
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.35.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

[INFO|modeling_utils.py:2993] 2023-10-05 03:48:11,811 >> loading weights file pytorch_model.bin from cache at /home/u131168/.cache/huggingface/hub/models--google--flan-t5-xl/snapshots/8772db7a7a11f7b08e6be7d7088f7a7fd4813bc5/pytorch_model.bin.index.json
[INFO|configuration_utils.py:770] 2023-10-05 03:48:11,824 >> Generate config GenerationConfig {
  "decoder_start_token_id": 0,
  "eos_token_id": 1,
  "pad_token_id": 0
}

-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:29<00:29, 29.10s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:35<00:00, 15.52s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:35<00:00, 17.56s/it]
[INFO|modeling_utils.py:3775] 2023-10-05 03:49:18,121 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.

[INFO|modeling_utils.py:3783] 2023-10-05 03:49:18,121 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at google/flan-t5-xl.
If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.
[INFO|configuration_utils.py:730] 2023-10-05 03:49:18,234 >> loading configuration file generation_config.json from cache at /home/u131168/.cache/huggingface/hub/models--google--flan-t5-xl/snapshots/8772db7a7a11f7b08e6be7d7088f7a7fd4813bc5/generation_config.json
[INFO|configuration_utils.py:770] 2023-10-05 03:49:18,235 >> Generate config GenerationConfig {
  "decoder_start_token_id": 0,
  "eos_token_id": 1,
  "pad_token_id": 0
}

[INFO|modeling_utils.py:1617] 2023-10-05 03:49:18,252 >> You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32100. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
[INFO|trainer.py:2123] 2023-10-05 03:49:33,396 >> Loading model from /home/u131168/mh_one_api/model/ft_models/flan-t5-xl_peft_ft_v1/checkpoint-14800.
[INFO|trainer.py:1760] 2023-10-05 03:50:04,061 >> ***** Running training *****
[INFO|trainer.py:1761] 2023-10-05 03:50:04,061 >>   Num examples = 395,082
[INFO|trainer.py:1762] 2023-10-05 03:50:04,061 >>   Num Epochs = 1
[INFO|trainer.py:1763] 2023-10-05 03:50:04,061 >>   Instantaneous batch size per device = 2
[INFO|trainer.py:1766] 2023-10-05 03:50:04,061 >>   Total train batch size (w. parallel, distributed & accumulation) = 2
[INFO|trainer.py:1767] 2023-10-05 03:50:04,061 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1768] 2023-10-05 03:50:04,061 >>   Total optimization steps = 197,541
[INFO|trainer.py:1769] 2023-10-05 03:50:04,073 >>   Number of trainable parameters = 4,718,592
[INFO|trainer.py:1789] 2023-10-05 03:50:04,083 >>   Continuing training from checkpoint, will skip to saved global_step
[INFO|trainer.py:1790] 2023-10-05 03:50:04,083 >>   Continuing training from epoch 0
[INFO|trainer.py:1791] 2023-10-05 03:50:04,083 >>   Continuing training from global step 14800
[INFO|trainer.py:1793] 2023-10-05 03:50:04,083 >>   Will skip the first 0 epochs then the first 14800 batches in the first epoch.
trainable params: 4,718,592 || all params: 2,854,361,088 || trainable%: 0.1653116706164963
-------------------traing-model
  0%|          | 0/197541 [00:00<?, ?it/s][WARNING|logging.py:290] 2023-10-05 03:50:04,178 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  7%|▋         | 14801/197541 [10:45<2:12:50, 22.93it/s]  7%|▋         | 14801/197541 [11:00<2:12:50, 22.93it/s]  7%|▋         | 14802/197541 [18:34<4:30:37, 11.25it/s]  7%|▋         | 14803/197541 [25:52<7:34:20,  6.70it/s]  7%|▋         | 14804/197541 [32:46<11:42:52,  4.33it/s]  7%|▋         | 14805/197541 [39:26<17:24:55,  2.91it/s]  7%|▋         | 14806/197541 [51:59<32:45:54,  1.55it/s]  7%|▋         | 14807/197541 [1:08:21<61:19:33,  1.21s/it]  7%|▋         | 14808/197541 [1:24:39<101:53:49,  2.01s/it]  7%|▋         | 14809/197541 [1:52:38<201:17:35,  3.97s/it]  7%|▋         | 14810/197541 [2:20:55<344:25:20,  6.79s/it]                                                              7%|▋         | 14810/197541 [2:20:55<344:25:20,  6.79s/it]  7%|▋         | 14811/197541 [2:48:41<544:17:40, 10.72s/it]  7%|▋         | 14812/197541 [3:10:14<764:14:16, 15.06s/it]  7%|▋         | 14813/197541 [3:31:33<1072:17:20, 21.13s/it]slurmstepd-idc-beta-batch-pvc-node-02: error: *** JOB 26873 ON idc-beta-batch-pvc-node-02 CANCELLED AT 2023-10-05T07:31:51 DUE TO TIME LIMIT ***
