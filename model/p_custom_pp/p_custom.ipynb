{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      ":: initializing oneAPI environment ...\n",
      "   bash: BASH_VERSION = 5.1.16(1)-release\n",
      "   args: Using \"$@\" for setvars.sh arguments: --force\n",
      ":: advisor -- latest\n",
      ":: ccl -- latest\n",
      ":: compiler -- latest\n",
      ":: dal -- latest\n",
      ":: debugger -- latest\n",
      ":: dev-utilities -- latest\n",
      ":: dnnl -- latest\n",
      ":: dpcpp-ct -- latest\n",
      ":: dpl -- latest\n",
      ":: embree -- latest\n",
      ":: inspector -- latest\n",
      ":: intelpython -- latest\n",
      "\n",
      "CommandNotFoundError: Your shell has not been properly configured to use 'conda deactivate'.\n",
      "To initialize your shell, run\n",
      "\n",
      "    $ conda init <SHELL_NAME>\n",
      "\n",
      "Currently supported shells are:\n",
      "  - bash\n",
      "  - fish\n",
      "  - tcsh\n",
      "  - xonsh\n",
      "  - zsh\n",
      "  - powershell\n",
      "\n",
      "See 'conda init --help' for more information and options.\n",
      "\n",
      "IMPORTANT: You may need to close and restart your shell after running 'conda init'.\n",
      "\n",
      "\n",
      "\n",
      "CommandNotFoundError: Your shell has not been properly configured to use 'conda deactivate'.\n",
      "To initialize your shell, run\n",
      "\n",
      "    $ conda init <SHELL_NAME>\n",
      "\n",
      "Currently supported shells are:\n",
      "  - bash\n",
      "  - fish\n",
      "  - tcsh\n",
      "  - xonsh\n",
      "  - zsh\n",
      "  - powershell\n",
      "\n",
      "See 'conda init --help' for more information and options.\n",
      "\n",
      "IMPORTANT: You may need to close and restart your shell after running 'conda init'.\n",
      "\n",
      "\n",
      "\n",
      "CommandNotFoundError: Your shell has not been properly configured to use 'conda deactivate'.\n",
      "To initialize your shell, run\n",
      "\n",
      "    $ conda init <SHELL_NAME>\n",
      "\n",
      "Currently supported shells are:\n",
      "  - bash\n",
      "  - fish\n",
      "  - tcsh\n",
      "  - xonsh\n",
      "  - zsh\n",
      "  - powershell\n",
      "\n",
      "See 'conda init --help' for more information and options.\n",
      "\n",
      "IMPORTANT: You may need to close and restart your shell after running 'conda init'.\n",
      "\n",
      "\n",
      ":: ipp -- latest\n",
      ":: ippcp -- latest\n",
      ":: ispc -- latest\n",
      ":: itac -- latest\n",
      ":: mkl -- latest\n",
      ":: modelzoo -- latest\n",
      ":: modin -- latest\n",
      ":: mpi -- latest\n",
      ":: neural-compressor -- latest\n",
      ":: oidn -- latest\n",
      ":: openpgl -- latest\n",
      ":: openvkl -- latest\n",
      ":: ospray -- latest\n",
      ":: ospray_studio -- latest\n",
      ":: pytorch -- latest\n",
      ":: rkcommon -- latest\n",
      ":: rkutil -- latest\n",
      ":: tbb -- latest\n",
      ":: tensorflow -- latest\n",
      ":: vtune -- latest\n",
      ":: oneAPI environment initialized ::\n",
      " \n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: intel-extension-for-pytorch in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (2.1.0)\n",
      "Requirement already satisfied: numpy in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from intel-extension-for-pytorch) (1.26.1)\n",
      "Requirement already satisfied: psutil in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from intel-extension-for-pytorch) (5.9.6)\n",
      "Requirement already satisfied: packaging in /home/common/miniconda3/envs/tensorflow_xpu/lib/python3.10/site-packages (from intel-extension-for-pytorch) (23.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (2.1.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: jinja2 in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: filelock in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from torch) (3.12.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from torch) (2.18.1)\n",
      "Requirement already satisfied: fsspec in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: networkx in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from torch) (3.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: sympy in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: typing-extensions in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.52)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: peft in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (0.5.0)\n",
      "Requirement already satisfied: psutil in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from peft) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: tqdm in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from peft) (4.66.1)\n",
      "Requirement already satisfied: accelerate in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from peft) (0.24.0)\n",
      "Requirement already satisfied: safetensors in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from peft) (0.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/common/miniconda3/envs/tensorflow_xpu/lib/python3.10/site-packages (from peft) (23.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from peft) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from peft) (1.26.1)\n",
      "Requirement already satisfied: transformers in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from peft) (4.34.1)\n",
      "Requirement already satisfied: typing-extensions in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (4.8.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: sympy in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (2.1.0)\n",
      "Requirement already satisfied: fsspec in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (2.18.1)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.3.1)\n",
      "Requirement already satisfied: jinja2 in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (8.9.2.26)\n",
      "Requirement already satisfied: networkx in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2)\n",
      "Requirement already satisfied: filelock in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.12.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft) (12.3.52)\n",
      "Requirement already satisfied: huggingface-hub in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from accelerate->peft) (0.17.3)\n",
      "Requirement already satisfied: requests in /home/common/miniconda3/envs/tensorflow_xpu/lib/python3.10/site-packages (from transformers->peft) (2.31.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from transformers->peft) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from transformers->peft) (0.14.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/common/miniconda3/envs/tensorflow_xpu/lib/python3.10/site-packages (from requests->transformers->peft) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/common/miniconda3/envs/tensorflow_xpu/lib/python3.10/site-packages (from requests->transformers->peft) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/common/miniconda3/envs/tensorflow_xpu/lib/python3.10/site-packages (from requests->transformers->peft) (2023.5.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/common/miniconda3/envs/tensorflow_xpu/lib/python3.10/site-packages (from requests->transformers->peft) (1.26.16)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# setting oneapi env and installing deps\n",
    "# !./p_custom2.sh\n",
    "\n",
    "# !python -m pip install mkl\n",
    "\n",
    "!source /opt/intel/oneapi/setvars.sh --force\n",
    "\n",
    "!pip install intel-extension-for-pytorch --no-cache-dir\n",
    "!pip install torch --no-cache-dir\n",
    "!pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uc4ddc6536e59d9d8f8f5069efdb4e25/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import time\n",
    "import intel_extension_for_pytorch as ipex\n",
    "\n",
    "from peft import AutoPeftModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'subprocess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/uc4ddc6536e59d9d8f8f5069efdb4e25/mh_one_api/model/p_custom_pp/p_custom.ipynb Cell 3\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bmh_one_api/home/uc4ddc6536e59d9d8f8f5069efdb4e25/mh_one_api/model/p_custom_pp/p_custom.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m pred_file_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfull_pred9.csv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bmh_one_api/home/uc4ddc6536e59d9d8f8f5069efdb4e25/mh_one_api/model/p_custom_pp/p_custom.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m pred_file_path\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/home/uc4ddc6536e59d9d8f8f5069efdb4e25/\u001b[39m\u001b[39m{\u001b[39;00mmh_dir\u001b[39m}\u001b[39;00m\u001b[39m/data/custom_pred/\u001b[39m\u001b[39m{\u001b[39;00mpred_file_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://tunnel%2Bmh_one_api/home/uc4ddc6536e59d9d8f8f5069efdb4e25/mh_one_api/model/p_custom_pp/p_custom.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m start_index\u001b[39m=\u001b[39msubprocess\u001b[39m.\u001b[39mcheck_output(\u001b[39m\"\u001b[39m\u001b[39mtail \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39mpred_file_path\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m -n 1 | awk -F\u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39m\u001b[39mprint $1}\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m,shell\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bmh_one_api/home/uc4ddc6536e59d9d8f8f5069efdb4e25/mh_one_api/model/p_custom_pp/p_custom.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(start_index)\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bmh_one_api/home/uc4ddc6536e59d9d8f8f5069efdb4e25/mh_one_api/model/p_custom_pp/p_custom.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m start_index,end_index\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\u001b[39m5\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'subprocess' is not defined"
     ]
    }
   ],
   "source": [
    "mh_dir='mh_one_api'\n",
    "with_intel_optimization=True\n",
    "without_intel_optimization=True\n",
    "\n",
    "pred_file_name=\"full_pred9.csv\"\n",
    "pred_file_path=f\"/home/uc4ddc6536e59d9d8f8f5069efdb4e25/{mh_dir}/data/custom_pred/{pred_file_name}\"\n",
    "start_index=subprocess.check_output(\"tail \"+pred_file_path+\" -n 1 | awk -F' ' '{print $1}'\",shell=True)\n",
    "print(start_index)\n",
    "start_index,end_index=0,5\n",
    "\n",
    "f_test_path=f\"/home/uc4ddc6536e59d9d8f8f5069efdb4e25/{mh_dir}/data/f_testd.csv\"\n",
    "f_test_data=pd.read_csv(f_test_path)\n",
    "\n",
    "\n",
    "checkpoint_dir=f\"/home/uc4ddc6536e59d9d8f8f5069efdb4e25/{mh_dir}/model/ft_models/flan-t5-xl_peft_ft_v2/\"\n",
    "checkpoint_name=subprocess.check_output(f\"ls {checkpoint_dir} | grep checkpoint | tail -1\",shell=True)\n",
    "checkpoint_name=str(checkpoint_name).replace(\"b'\",\"\").replace(\"\\\\n'\",\"\")\n",
    "checkpoint_path=checkpoint_dir+checkpoint_name\n",
    "model_path=checkpoint_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:34<00:00, 17.11s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xl\")\n",
    "model = AutoPeftModelForSeq2SeqLM.from_pretrained(model_path, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "PyTorch is not linked with support for xpu devices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/uc4ddc6536e59d9d8f8f5069efdb4e25/mh_one_api/model/p_custom_pp/p_custom.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bmh_one_api/home/uc4ddc6536e59d9d8f8f5069efdb4e25/mh_one_api/model/p_custom_pp/p_custom.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mif\u001b[39;00m with_intel_optimization\u001b[39m==\u001b[39m\u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bmh_one_api/home/uc4ddc6536e59d9d8f8f5069efdb4e25/mh_one_api/model/p_custom_pp/p_custom.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mglobal\u001b[39;00m imodel\n\u001b[0;32m----> <a href='vscode-notebook-cell://tunnel%2Bmh_one_api/home/uc4ddc6536e59d9d8f8f5069efdb4e25/mh_one_api/model/p_custom_pp/p_custom.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     imodel \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mto(\u001b[39m'\u001b[39;49m\u001b[39mxpu\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bmh_one_api/home/uc4ddc6536e59d9d8f8f5069efdb4e25/mh_one_api/model/p_custom_pp/p_custom.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     imodel \u001b[39m=\u001b[39m ipex\u001b[39m.\u001b[39moptimize(imodel)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1158\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1160\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    812\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    812\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    812\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m   1156\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1158\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: PyTorch is not linked with support for xpu devices"
     ]
    }
   ],
   "source": [
    "\n",
    "if with_intel_optimization==True:\n",
    "    global imodel\n",
    "    # imodel = model.to('xpu')\n",
    "    imodel = ipex.optimize(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------running inference on finetuned model---------------\n",
      "-----------------metrics with vs without intel oneapi optimization------------\n",
      "---inference time without intel oneapi optimization: 113.74230933189392 seconds ---\n",
      "---inference time with intel oneapi optimization: 99.55642342567444 seconds ---\n",
      "---inference time reduced (%) by using intel one api optimization: 12.471951720995776 % ---\n"
     ]
    }
   ],
   "source": [
    "# print(subprocess.check_output(\". /opt/intel/oneapi/setvars.sh \",shell=True))\n",
    "# print(subprocess.check_output(\"sycl-ls\",shell=True))\n",
    "# print(subprocess.check_output(\"./sys_info.sh\"))\n",
    "\n",
    "print(\"----------------------running inference on finetuned model---------------\")\n",
    "print(\"-----------------metrics with vs without intel oneapi optimization------------\")\n",
    "print(\"---inference time without intel oneapi optimization: %s seconds ---\" % (pt))\n",
    "print(\"---inference time with intel oneapi optimization: %s seconds ---\" % (pti))\n",
    "print(f\"---inference time reduced (%) by using intel one api optimization: {(pt-pti)*100/pt} % ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting 0 to 0 prompt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------time taken by normal model=113.90422749519348 seconds\n",
      "--------------time taken by optimized model=100.90329360961914 seconds\n",
      "-----------------metrics with vs without intel oneapi optimization------------\n",
      "---inference time without intel oneapi optimization: 113.90422749519348 seconds ---\n",
      "---inference time with intel oneapi optimization: 100.90329360961914 seconds ---\n",
      "---inference time reduced (%) by using intel one api optimization: 11.413916912015361 % ---\n",
      "\n",
      "-------------wrote 0 to 0 preds\n",
      "predicting 1 to 1 prompt\n",
      "--------------time taken by normal model=116.72430205345154 seconds\n",
      "--------------time taken by optimized model=137.21207332611084 seconds\n",
      "-----------------metrics with vs without intel oneapi optimization------------\n",
      "---inference time without intel oneapi optimization: 116.72430205345154 seconds ---\n",
      "---inference time with intel oneapi optimization: 137.21207332611084 seconds ---\n",
      "---inference time reduced (%) by using intel one api optimization: -17.552275672016734 % ---\n",
      "\n",
      "-------------wrote 1 to 1 preds\n",
      "predicting 2 to 2 prompt\n",
      "--------------time taken by normal model=87.77246141433716 seconds\n",
      "--------------time taken by optimized model=87.20250463485718 seconds\n",
      "-----------------metrics with vs without intel oneapi optimization------------\n",
      "---inference time without intel oneapi optimization: 87.77246141433716 seconds ---\n",
      "---inference time with intel oneapi optimization: 87.20250463485718 seconds ---\n",
      "---inference time reduced (%) by using intel one api optimization: 0.6493571791150443 % ---\n",
      "\n",
      "-------------wrote 2 to 2 preds\n",
      "predicting 3 to 3 prompt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/uc4ddc6536e59d9d8f8f5069efdb4e25/mh_one_api/model/p_custom_pp/p_custom.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bmh_one_api/home/uc4ddc6536e59d9d8f8f5069efdb4e25/mh_one_api/model/p_custom_pp/p_custom.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mif\u001b[39;00m without_intel_optimization\u001b[39m==\u001b[39m\u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bmh_one_api/home/uc4ddc6536e59d9d8f8f5069efdb4e25/mh_one_api/model/p_custom_pp/p_custom.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> <a href='vscode-notebook-cell://tunnel%2Bmh_one_api/home/uc4ddc6536e59d9d8f8f5069efdb4e25/mh_one_api/model/p_custom_pp/p_custom.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(input_ids\u001b[39m=\u001b[39;49minput_ids, do_sample\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, max_length\u001b[39m=\u001b[39;49m\u001b[39m150\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bmh_one_api/home/uc4ddc6536e59d9d8f8f5069efdb4e25/mh_one_api/model/p_custom_pp/p_custom.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     pt\u001b[39m=\u001b[39mtime\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bmh_one_api/home/uc4ddc6536e59d9d8f8f5069efdb4e25/mh_one_api/model/p_custom_pp/p_custom.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m--------------time taken by normal model=\u001b[39m\u001b[39m{\u001b[39;00mpt\u001b[39m}\u001b[39;00m\u001b[39m seconds\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/peft/peft_model.py:1190\u001b[0m, in \u001b[0;36mPeftModelForSeq2SeqLM.generate\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1189\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m peft_config\u001b[39m.\u001b[39mis_prompt_learning:\n\u001b[0;32m-> 1190\u001b[0m         outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_model\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1192\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m kwargs:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1496\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1488\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m   1489\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mA decoder-only architecture is being used, but right-padding was detected! For correct \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1490\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mgeneration results, please set `padding_side=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m\u001b[39m` when initializing the tokenizer.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1491\u001b[0m         )\n\u001b[1;32m   1493\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mencoder_outputs\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m model_kwargs:\n\u001b[1;32m   1494\u001b[0m     \u001b[39m# if model is encoder decoder encoder_outputs are created\u001b[39;00m\n\u001b[1;32m   1495\u001b[0m     \u001b[39m# and added to `model_kwargs`\u001b[39;00m\n\u001b[0;32m-> 1496\u001b[0m     model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_encoder_decoder_kwargs_for_generation(\n\u001b[1;32m   1497\u001b[0m         inputs_tensor, model_kwargs, model_input_name\n\u001b[1;32m   1498\u001b[0m     )\n\u001b[1;32m   1500\u001b[0m \u001b[39m# 5. Prepare `input_ids` which will be used for auto-regressive generation\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py:661\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_encoder_decoder_kwargs_for_generation\u001b[0;34m(self, inputs_tensor, model_kwargs, model_input_name)\u001b[0m\n\u001b[1;32m    659\u001b[0m encoder_kwargs[\u001b[39m\"\u001b[39m\u001b[39mreturn_dict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    660\u001b[0m encoder_kwargs[model_input_name] \u001b[39m=\u001b[39m inputs_tensor\n\u001b[0;32m--> 661\u001b[0m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39mencoder_outputs\u001b[39m\u001b[39m\"\u001b[39m]: ModelOutput \u001b[39m=\u001b[39m encoder(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mencoder_kwargs)\n\u001b[1;32m    663\u001b[0m \u001b[39mreturn\u001b[39;00m model_kwargs\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1123\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1110\u001b[0m     layer_outputs \u001b[39m=\u001b[39m checkpoint(\n\u001b[1;32m   1111\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m   1112\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1120\u001b[0m         \u001b[39mNone\u001b[39;00m,  \u001b[39m# past_key_value is always None with gradient checkpointing\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     )\n\u001b[1;32m   1122\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1123\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m   1124\u001b[0m         hidden_states,\n\u001b[1;32m   1125\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   1126\u001b[0m         position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[1;32m   1127\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1128\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   1129\u001b[0m         encoder_decoder_position_bias\u001b[39m=\u001b[39;49mencoder_decoder_position_bias,\n\u001b[1;32m   1130\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[1;32m   1131\u001b[0m         cross_attn_layer_head_mask\u001b[39m=\u001b[39;49mcross_attn_layer_head_mask,\n\u001b[1;32m   1132\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m   1133\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1134\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1135\u001b[0m     )\n\u001b[1;32m   1137\u001b[0m \u001b[39m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[39m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:755\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    752\u001b[0m     attention_outputs \u001b[39m=\u001b[39m attention_outputs \u001b[39m+\u001b[39m cross_attention_outputs[\u001b[39m2\u001b[39m:]\n\u001b[1;32m    754\u001b[0m \u001b[39m# Apply Feed Forward layer\u001b[39;00m\n\u001b[0;32m--> 755\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m](hidden_states)\n\u001b[1;32m    757\u001b[0m \u001b[39m# clamp inf values to enable fp16 training\u001b[39;00m\n\u001b[1;32m    758\u001b[0m \u001b[39mif\u001b[39;00m hidden_states\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39mfloat16:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:344\u001b[0m, in \u001b[0;36mT5LayerFF.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states):\n\u001b[1;32m    343\u001b[0m     forwarded_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 344\u001b[0m     forwarded_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mDenseReluDense(forwarded_states)\n\u001b[1;32m    345\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_states \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(forwarded_states)\n\u001b[1;32m    346\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:312\u001b[0m, in \u001b[0;36mT5DenseGatedActDense.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states):\n\u001b[0;32m--> 312\u001b[0m     hidden_gelu \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwi_0(hidden_states))\n\u001b[1;32m    313\u001b[0m     hidden_linear \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwi_1(hidden_states)\n\u001b[1;32m    314\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_gelu \u001b[39m*\u001b[39m hidden_linear\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# ------predict--------------\n",
    "bs=1\n",
    "end_index=len(f_test_data)\n",
    "for i in range(start_index,end_index,bs):\n",
    "    print(f\"predicting {i} to {i+bs-1} prompt\")\n",
    "    prompts = f_test_data.loc[i:i+bs-1,['input','instruction']].values.tolist()\n",
    "    prompts=prompts\n",
    "    t_prompts=[]\n",
    "    for p in prompts:\n",
    "        context=str(p[0])\n",
    "        # context=str(p[0])\n",
    "        question=p[1]\n",
    "        t_prompts.append([context,question])\n",
    "        # t_prompts+=[f\"input: {context}\\n\\ninstruction: {question}\"]\n",
    "    prompts=t_prompts\n",
    "        \n",
    "    # print(prompts)\n",
    "    res=[]\n",
    "    input_ids = tokenizer(prompts, return_tensors=\"pt\" ,padding=True,truncation=True, max_length=512).input_ids\n",
    "    # sample up to 30 tokens\n",
    "    torch.manual_seed(0)  # doctest: +IGNORE_RESULT\n",
    "\n",
    "    if without_intel_optimization==True:\n",
    "        start_time = time.time()\n",
    "        outputs = model.generate(input_ids=input_ids, do_sample=True, max_length=150)\n",
    "        pt=time.time() - start_time\n",
    "        print(f\"--------------time taken by normal model={pt} seconds\")\n",
    "\n",
    "\n",
    "    if with_intel_optimization==True:\n",
    "        # input_ids=input_ids.to('xpu')\n",
    "        start_time = time.time()\n",
    "        outputs = imodel.generate(input_ids=input_ids, do_sample=True, max_length=150)\n",
    "        pti=time.time() - start_time\n",
    "        print(f\"--------------time taken by optimized model={pti} seconds\")\n",
    "\n",
    "        if without_intel_optimization==True:\n",
    "            # print(\"---------------system info---------------------------\")\n",
    "            # print(subprocess.check_output(\"source /opt/intel/oneapi/setvars.sh && sycl-ls\",shell=True))\n",
    "            print(\"-----------------metrics with vs without intel oneapi optimization------------\")\n",
    "            print(\"---inference time without intel oneapi optimization: %s seconds ---\" % (pt))\n",
    "            print(\"---inference time with intel oneapi optimization: %s seconds ---\" % (pti))\n",
    "            print(f\"---inference time reduced (%) by using intel one api optimization: {(pt-pti)*100/pt} % ---\")\n",
    "\n",
    "\n",
    "    res+=tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        \n",
    "    # Writing data to a file\n",
    "    with open(pred_file_path, \"a+\") as file1:\n",
    "        file1.writelines(f\"{i+i1} $$ {res[i1]}\\n\" for i1 in range(len(res)))\n",
    "    print(f\"\\n-------------wrote {i} to {i+bs-1} preds\")\n",
    "\n",
    "print(\"-----------------Prediction_finished-----------------------\")\n",
    "# print(subprocess.check_output(\"scancel $((SLURM_JOB_ID+1))\",shell=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
