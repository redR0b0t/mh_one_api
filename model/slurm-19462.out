starting fine tuning model on mutliple nodes
Requirement already satisfied: datasets in /home/u131168/.local/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (2.14.5)
Collecting torch
  Using cached torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)
Collecting transformers>=4.32.0
  Using cached transformers-4.33.2-py3-none-any.whl (7.6 MB)
Collecting sentencepiece
  Using cached sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)
Collecting peft
  Using cached peft-0.5.0-py3-none-any.whl (85 kB)
Collecting evaluate
  Using cached evaluate-0.4.0-py3-none-any.whl (81 kB)
Collecting nltk
  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)
Collecting rouge_score
  Using cached rouge_score-0.1.2-py3-none-any.whl
Collecting einops
  Using cached einops-0.6.1-py3-none-any.whl (42 kB)
Requirement already satisfied: pandas in /home/u131168/.local/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (2.1.0)
Requirement already satisfied: requests>=2.19.0 in /opt/miniconda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (2.28.1)
Requirement already satisfied: packaging in /opt/miniconda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (23.0)
Requirement already satisfied: xxhash in /home/u131168/.local/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (3.3.0)
Requirement already satisfied: aiohttp in /home/u131168/.local/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (3.8.5)
Requirement already satisfied: pyyaml>=5.1 in /home/u131168/.local/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (6.0.1)
Requirement already satisfied: pyarrow>=8.0.0 in /home/u131168/.local/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (13.0.0)
Requirement already satisfied: numpy>=1.17 in /home/u131168/.local/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (1.26.0)
Requirement already satisfied: multiprocess in /home/u131168/.local/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (0.70.15)
Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /home/u131168/.local/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (2023.6.0)
Requirement already satisfied: tqdm>=4.62.1 in /opt/miniconda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (4.65.0)
Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/u131168/.local/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (0.3.7)
Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /home/u131168/.local/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (0.17.2)
Collecting nvidia-cusolver-cu11==11.4.0.1
  Using cached nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)
Collecting nvidia-cufft-cu11==10.9.0.58
  Using cached nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)
Collecting nvidia-cuda-cupti-cu11==11.7.101
  Using cached nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)
Collecting nvidia-nvtx-cu11==11.7.91
  Using cached nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)
Collecting nvidia-cuda-nvrtc-cu11==11.7.99
  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)
Collecting nvidia-cuda-runtime-cu11==11.7.99
  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)
Collecting nvidia-cusparse-cu11==11.7.4.91
  Using cached nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)
Requirement already satisfied: filelock in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (3.12.4)
Collecting nvidia-nccl-cu11==2.14.3
  Using cached nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)
Collecting nvidia-cublas-cu11==11.10.3.66
  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)
Requirement already satisfied: jinja2 in /opt/miniconda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (3.1.2)
Collecting nvidia-cudnn-cu11==8.5.0.96
  Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)
Collecting nvidia-curand-cu11==10.2.10.91
  Using cached nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)
Collecting sympy
  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)
Collecting triton==2.0.0
  Using cached triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)
Collecting networkx
  Using cached networkx-3.1-py3-none-any.whl (2.1 MB)
Requirement already satisfied: typing-extensions in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (4.8.0)
Requirement already satisfied: wheel in /opt/miniconda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->-r requirements.txt (line 2)) (0.38.4)
Requirement already satisfied: setuptools in /opt/miniconda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->-r requirements.txt (line 2)) (65.6.3)
Collecting lit
  Using cached lit-16.0.6-py3-none-any.whl
Collecting cmake
  Using cached cmake-3.27.5-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.1 MB)
Collecting safetensors>=0.3.1
  Using cached safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)
Collecting tokenizers!=0.11.3,<0.14,>=0.11.1
  Using cached tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)
Collecting regex!=2019.12.17
  Using cached regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB)
Requirement already satisfied: psutil in /opt/miniconda/lib/python3.10/site-packages (from peft->-r requirements.txt (line 5)) (5.9.0)
Collecting accelerate
  Using cached accelerate-0.23.0-py3-none-any.whl (258 kB)
Collecting responses<0.19
  Using cached responses-0.18.0-py3-none-any.whl (38 kB)
Collecting click
  Using cached click-8.1.7-py3-none-any.whl (97 kB)
Collecting joblib
  Using cached joblib-1.3.2-py3-none-any.whl (302 kB)
Requirement already satisfied: six>=1.14.0 in /opt/miniconda/lib/python3.10/site-packages (from rouge_score->-r requirements.txt (line 8)) (1.16.0)
Collecting absl-py
  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)
Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/miniconda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (2.0.4)
Requirement already satisfied: frozenlist>=1.1.1 in /home/u131168/.local/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.4.0)
Requirement already satisfied: aiosignal>=1.1.2 in /home/u131168/.local/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.3.1)
Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/u131168/.local/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (4.0.3)
Requirement already satisfied: yarl<2.0,>=1.0 in /home/u131168/.local/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.9.2)
Requirement already satisfied: attrs>=17.3.0 in /home/u131168/.local/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (23.1.0)
Requirement already satisfied: multidict<7.0,>=4.5 in /home/u131168/.local/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (6.0.4)
Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda/lib/python3.10/site-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 1)) (2023.5.7)
Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda/lib/python3.10/site-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 1)) (3.4)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/miniconda/lib/python3.10/site-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 1)) (1.26.15)
Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda/lib/python3.10/site-packages (from jinja2->torch->-r requirements.txt (line 2)) (2.1.1)
Requirement already satisfied: python-dateutil>=2.8.2 in /home/u131168/.local/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2.8.2)
Requirement already satisfied: tzdata>=2022.1 in /home/u131168/.local/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2023.3)
Requirement already satisfied: pytz>=2020.1 in /home/u131168/.local/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2023.3.post1)
Collecting mpmath>=0.19
  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)
Installing collected packages: tokenizers, sentencepiece, safetensors, mpmath, lit, cmake, sympy, regex, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, networkx, joblib, einops, click, absl-py, responses, nvidia-cusolver-cu11, nvidia-cudnn-cu11, nltk, transformers, rouge_score, evaluate, triton, torch, accelerate, peft
Successfully installed absl-py-1.4.0 accelerate-0.23.0 click-8.1.7 cmake-3.27.5 einops-0.6.1 evaluate-0.4.0 joblib-1.3.2 lit-16.0.6 mpmath-1.3.0 networkx-3.1 nltk-3.8.1 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 peft-0.5.0 regex-2023.8.8 responses-0.18.0 rouge_score-0.1.2 safetensors-0.3.3 sentencepiece-0.1.99 sympy-1.12 tokenizers-0.13.3 torch-2.0.1 transformers-4.33.2 triton-2.0.0
-------------------starting-script
-------------------setting up-logging
09/19/2023 01:02:53 - WARNING - __main__ - Process rank: 0, device: cpu, n_gpu: 0distributed training: True, 16-bits training: False
09/19/2023 01:02:53 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=0,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/u131168/mh_one_api/model/ft_models/flan-t5-xl_peft_finetuned_model/runs/Sep19_01-02-53_idc-beta-batch-pvc-node-12,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/u131168/mh_one_api/model/ft_models/flan-t5-xl_peft_finetuned_model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=2,
per_device_train_batch_size=2,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/home/u131168/mh_one_api/model/ft_models/flan-t5-xl_peft_finetuned_model,
save_on_each_node=False,
save_safetensors=False,
save_steps=2000,
save_strategy=steps,
save_total_limit=2,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
sortish_sampler=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
/home/u131168/.local/lib/python3.10/site-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=None' instead.
  warnings.warn(
Using custom data configuration default-fd3cb62fb7f66139
----------------------detecting checkpoint
09/19/2023 01:02:54 - INFO - datasets.builder - Using custom data configuration default-fd3cb62fb7f66139
Loading Dataset Infos from /home/u131168/.local/lib/python3.10/site-packages/datasets/packaged_modules/csv
09/19/2023 01:02:54 - INFO - datasets.info - Loading Dataset Infos from /home/u131168/.local/lib/python3.10/site-packages/datasets/packaged_modules/csv
Overwrite dataset info from restored data version if exists.
09/19/2023 01:02:54 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/u131168/.cache/huggingface/datasets/csv/default-fd3cb62fb7f66139/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d
09/19/2023 01:02:54 - INFO - datasets.info - Loading Dataset info from /home/u131168/.cache/huggingface/datasets/csv/default-fd3cb62fb7f66139/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d
Found cached dataset csv (/home/u131168/.cache/huggingface/datasets/csv/default-fd3cb62fb7f66139/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)
09/19/2023 01:02:54 - INFO - datasets.builder - Found cached dataset csv (/home/u131168/.cache/huggingface/datasets/csv/default-fd3cb62fb7f66139/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)
Loading Dataset info from /home/u131168/.cache/huggingface/datasets/csv/default-fd3cb62fb7f66139/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d
09/19/2023 01:02:54 - INFO - datasets.info - Loading Dataset info from /home/u131168/.cache/huggingface/datasets/csv/default-fd3cb62fb7f66139/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d
[INFO|tokenization_utils_base.py:1852] 2023-09-19 01:02:54,776 >> loading file spiece.model from cache at /home/u131168/.cache/huggingface/hub/models--google--flan-t5-small/snapshots/2d036ee774a9cb8d7e03c9f2e78ae0a16343a9d9/spiece.model
[INFO|tokenization_utils_base.py:1852] 2023-09-19 01:02:54,776 >> loading file tokenizer.json from cache at /home/u131168/.cache/huggingface/hub/models--google--flan-t5-small/snapshots/2d036ee774a9cb8d7e03c9f2e78ae0a16343a9d9/tokenizer.json
[INFO|tokenization_utils_base.py:1852] 2023-09-19 01:02:54,776 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1852] 2023-09-19 01:02:54,776 >> loading file special_tokens_map.json from cache at /home/u131168/.cache/huggingface/hub/models--google--flan-t5-small/snapshots/2d036ee774a9cb8d7e03c9f2e78ae0a16343a9d9/special_tokens_map.json
[INFO|tokenization_utils_base.py:1852] 2023-09-19 01:02:54,776 >> loading file tokenizer_config.json from cache at /home/u131168/.cache/huggingface/hub/models--google--flan-t5-small/snapshots/2d036ee774a9cb8d7e03c9f2e78ae0a16343a9d9/tokenizer_config.json
Loading cached processed dataset at /home/u131168/.cache/huggingface/datasets/csv/default-fd3cb62fb7f66139/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-6a813a49f6016fad.arrow
09/19/2023 01:02:54 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/u131168/.cache/huggingface/datasets/csv/default-fd3cb62fb7f66139/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-6a813a49f6016fad.arrow
[INFO|configuration_utils.py:715] 2023-09-19 01:02:55,679 >> loading configuration file config.json from cache at /home/u131168/.cache/huggingface/hub/models--google--flan-t5-small/snapshots/2d036ee774a9cb8d7e03c9f2e78ae0a16343a9d9/config.json
[INFO|configuration_utils.py:775] 2023-09-19 01:02:55,682 >> Model config T5Config {
  "_name_or_path": "google/flan-t5-small",
  "architectures": [
    "T5ForConditionalGeneration"
  ],
  "classifier_dropout": 0.0,
  "d_ff": 1024,
  "d_kv": 64,
  "d_model": 512,
  "decoder_start_token_id": 0,
  "dense_act_fn": "gelu_new",
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "gated-gelu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "t5",
  "n_positions": 512,
  "num_decoder_layers": 8,
  "num_heads": 6,
  "num_layers": 8,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "task_specific_params": {
    "summarization": {
      "early_stopping": true,
      "length_penalty": 2.0,
      "max_length": 200,
      "min_length": 30,
      "no_repeat_ngram_size": 3,
      "num_beams": 4,
      "prefix": "summarize: "
    },
    "translation_en_to_de": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to German: "
    },
    "translation_en_to_fr": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to French: "
    },
    "translation_en_to_ro": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to Romanian: "
    }
  },
  "tie_word_embeddings": false,
  "transformers_version": "4.33.2",
  "use_cache": true,
  "vocab_size": 32128
}

[INFO|modeling_utils.py:2869] 2023-09-19 01:02:55,693 >> loading weights file pytorch_model.bin from cache at /home/u131168/.cache/huggingface/hub/models--google--flan-t5-small/snapshots/2d036ee774a9cb8d7e03c9f2e78ae0a16343a9d9/pytorch_model.bin
[INFO|configuration_utils.py:768] 2023-09-19 01:02:56,220 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "decoder_start_token_id": 0,
  "eos_token_id": 1,
  "pad_token_id": 0,
  "transformers_version": "4.33.2"
}

[INFO|modeling_utils.py:3655] 2023-09-19 01:02:56,729 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.

[INFO|modeling_utils.py:3663] 2023-09-19 01:02:56,729 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at google/flan-t5-small.
If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.
[INFO|configuration_utils.py:730] 2023-09-19 01:02:56,818 >> loading configuration file generation_config.json from cache at /home/u131168/.cache/huggingface/hub/models--google--flan-t5-small/snapshots/2d036ee774a9cb8d7e03c9f2e78ae0a16343a9d9/generation_config.json
[INFO|configuration_utils.py:768] 2023-09-19 01:02:56,818 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "decoder_start_token_id": 0,
  "eos_token_id": 1,
  "pad_token_id": 0,
  "transformers_version": "4.33.2"
}

[WARNING|modeling_utils.py:1519] 2023-09-19 01:02:56,823 >> You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32100. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
[INFO|trainer.py:1712] 2023-09-19 01:02:57,142 >> ***** Running training *****
[INFO|trainer.py:1713] 2023-09-19 01:02:57,142 >>   Num examples = 66,611
[INFO|trainer.py:1714] 2023-09-19 01:02:57,142 >>   Num Epochs = 5
[INFO|trainer.py:1715] 2023-09-19 01:02:57,142 >>   Instantaneous batch size per device = 2
[INFO|trainer.py:1718] 2023-09-19 01:02:57,142 >>   Total train batch size (w. parallel, distributed & accumulation) = 2
[INFO|trainer.py:1719] 2023-09-19 01:02:57,142 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1720] 2023-09-19 01:02:57,142 >>   Total optimization steps = 166,530
[INFO|trainer.py:1721] 2023-09-19 01:02:57,143 >>   Number of trainable parameters = 344,064
trainable params: 344,064 || all params: 77,276,544 || trainable%: 0.44523730253775323
-------------------traing-model
  0%|          | 0/166530 [00:00<?, ?it/s][WARNING|logging.py:290] 2023-09-19 01:02:57,153 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  0%|          | 1/166530 [00:00<12:02:37,  3.84it/s]  0%|          | 2/166530 [00:00<11:03:16,  4.18it/s]  0%|          | 3/166530 [00:00<10:37:10,  4.36it/s]  0%|          | 4/166530 [00:00<11:06:22,  4.16it/s]  0%|          | 5/166530 [00:01<10:25:56,  4.43it/s]  0%|          | 6/166530 [00:01<11:09:24,  4.15it/s]  0%|          | 7/166530 [00:01<10:51:29,  4.26it/s]  0%|          | 8/166530 [00:01<10:29:50,  4.41it/s]  0%|          | 9/166530 [00:02<11:21:57,  4.07it/s]  0%|          | 10/166530 [00:02<11:23:03,  4.06it/s]                                                        0%|          | 10/166530 [00:02<11:23:03,  4.06it/s]  0%|          | 11/166530 [00:02<11:15:29,  4.11it/s]  0%|          | 12/166530 [00:02<11:14:58,  4.11it/s]  0%|          | 13/166530 [00:03<11:01:49,  4.19it/s]  0%|          | 14/166530 [00:03<11:25:35,  4.05it/s]  0%|          | 15/166530 [00:03<11:01:54,  4.19it/s]  0%|          | 16/166530 [00:03<10:29:15,  4.41it/s]  0%|          | 17/166530 [00:03<10:12:39,  4.53it/s]  0%|          | 18/166530 [00:04<10:08:33,  4.56it/s]  0%|          | 19/166530 [00:04<10:44:55,  4.30it/s]  0%|          | 20/166530 [00:05<16:20:10,  2.83it/s]                                                        0%|          | 20/166530 [00:05<16:20:10,  2.83it/s]  0%|          | 21/166530 [00:05<15:50:47,  2.92it/s]  0%|          | 22/166530 [00:05<14:49:00,  3.12it/s]  0%|          | 23/166530 [00:05<14:10:12,  3.26it/s]  0%|          | 24/166530 [00:06<13:01:05,  3.55it/s]  0%|          | 25/166530 [00:06<12:43:34,  3.63it/s]  0%|          | 26/166530 [00:06<12:05:11,  3.83it/s]  0%|          | 27/166530 [00:06<11:27:19,  4.04it/s]  0%|          | 28/166530 [00:07<11:22:37,  4.07it/s]  0%|          | 29/166530 [00:07<10:44:26,  4.31it/s]  0%|          | 30/166530 [00:07<10:19:37,  4.48it/s]                                                        0%|          | 30/166530 [00:07<10:19:37,  4.48it/s]  0%|          | 31/166530 [00:07<10:03:42,  4.60it/s]  0%|          | 32/166530 [00:07<10:12:38,  4.53it/s]  0%|          | 33/166530 [00:08<10:21:55,  4.46it/s]  0%|          | 34/166530 [00:08<10:40:08,  4.33it/s]  0%|          | 35/166530 [00:08<10:46:12,  4.29it/s]  0%|          | 36/166530 [00:08<10:44:18,  4.31it/s]  0%|          | 37/166530 [00:09<10:40:21,  4.33it/s]  0%|          | 38/166530 [00:09<10:37:53,  4.35it/s]  0%|          | 39/166530 [00:09<10:48:16,  4.28it/s]  0%|          | 40/166530 [00:09<10:55:10,  4.24it/s]                                                        0%|          | 40/166530 [00:09<10:55:10,  4.24it/s]  0%|          | 41/166530 [00:10<11:34:13,  4.00it/s]  0%|          | 42/166530 [00:10<11:23:58,  4.06it/s]  0%|          | 43/166530 [00:10<10:47:54,  4.28it/s]  0%|          | 44/166530 [00:10<10:46:44,  4.29it/s]  0%|          | 45/166530 [00:11<10:47:25,  4.29it/s]  0%|          | 46/166530 [00:11<10:35:09,  4.37it/s]  0%|          | 47/166530 [00:11<10:05:48,  4.58it/s]  0%|          | 48/166530 [00:11<9:47:39,  4.72it/s]   0%|          | 49/166530 [00:11<9:43:16,  4.76it/s]  0%|          | 50/166530 [00:12<9:30:12,  4.87it/s]                                                       0%|          | 50/166530 [00:12<9:30:12,  4.87it/s]  0%|          | 51/166530 [00:12<9:34:28,  4.83it/s]  0%|          | 52/166530 [00:12<10:17:13,  4.50it/s]  0%|          | 53/166530 [00:12<10:05:36,  4.58it/s]  0%|          | 54/166530 [00:12<9:44:24,  4.75it/s]   0%|          | 55/166530 [00:13<9:39:36,  4.79it/s]  0%|          | 56/166530 [00:13<9:41:52,  4.77it/s]  0%|          | 57/166530 [00:13<9:37:09,  4.81it/s]  0%|          | 58/166530 [00:13<9:59:09,  4.63it/s]  0%|          | 59/166530 [00:13<9:49:18,  4.71it/s]  0%|          | 60/166530 [00:14<9:50:20,  4.70it/s]                                                       0%|          | 60/166530 [00:14<9:50:20,  4.70it/s]  0%|          | 61/166530 [00:14<9:50:50,  4.70it/s]  0%|          | 62/166530 [00:14<9:45:09,  4.74it/s]  0%|          | 63/166530 [00:14<10:07:26,  4.57it/s]  0%|          | 64/166530 [00:15<10:15:31,  4.51it/s]  0%|          | 65/166530 [00:15<10:25:54,  4.43it/s]  0%|          | 66/166530 [00:15<10:11:16,  4.54it/s]  0%|          | 67/166530 [00:15<10:18:28,  4.49it/s]  0%|          | 68/166530 [00:15<10:02:52,  4.60it/s]  0%|          | 69/166530 [00:16<10:05:13,  4.58it/s]  0%|          | 70/166530 [00:16<10:10:25,  4.54it/s]                                                        0%|          | 70/166530 [00:16<10:10:25,  4.54it/s]  0%|          | 71/166530 [00:16<10:48:35,  4.28it/s]  0%|          | 72/166530 [00:16<10:40:28,  4.33it/s]  0%|          | 73/166530 [00:17<10:35:56,  4.36it/s]  0%|          | 74/166530 [00:17<10:24:57,  4.44it/s]  0%|          | 75/166530 [00:17<9:58:42,  4.63it/s]   0%|          | 76/166530 [00:17<10:11:09,  4.54it/s]  0%|          | 77/166530 [00:17<10:03:45,  4.59it/s]  0%|          | 78/166530 [00:18<10:02:48,  4.60it/s]  0%|          | 79/166530 [00:18<10:04:59,  4.59it/s]  0%|          | 80/166530 [00:18<10:32:07,  4.39it/s]                                                        0%|          | 80/166530 [00:18<10:32:07,  4.39it/s]  0%|          | 81/166530 [00:18<10:51:13,  4.26it/s]  0%|          | 82/166530 [00:19<11:40:29,  3.96it/s]  0%|          | 83/166530 [00:19<12:05:37,  3.82it/s]  0%|          | 84/166530 [00:19<12:29:24,  3.70it/s]  0%|          | 85/166530 [00:20<12:39:46,  3.65it/s]  0%|          | 86/166530 [00:20<12:48:50,  3.61it/s]  0%|          | 87/166530 [00:20<14:32:46,  3.18it/s]  0%|          | 88/166530 [00:21<13:58:29,  3.31it/s]  0%|          | 89/166530 [00:21<14:18:20,  3.23it/s]  0%|          | 90/166530 [00:21<13:04:45,  3.53it/s]                                                        0%|          | 90/166530 [00:21<13:04:45,  3.53it/s]  0%|          | 91/166530 [00:21<12:13:55,  3.78it/s]  0%|          | 92/166530 [00:22<12:13:37,  3.78it/s]  0%|          | 93/166530 [00:22<12:01:21,  3.85it/s]  0%|          | 94/166530 [00:22<13:00:28,  3.55it/s]  0%|          | 95/166530 [00:22<12:48:46,  3.61it/s]  0%|          | 96/166530 [00:23<13:15:08,  3.49it/s]  0%|          | 97/166530 [00:23<12:45:27,  3.62it/s]  0%|          | 98/166530 [00:23<12:51:38,  3.59it/s]  0%|          | 99/166530 [00:24<12:53:55,  3.58it/s]  0%|          | 100/166530 [00:24<12:57:29,  3.57it/s]                                                         0%|          | 100/166530 [00:24<12:57:29,  3.57it/s]  0%|          | 101/166530 [00:24<13:03:15,  3.54it/s]  0%|          | 102/166530 [00:24<13:25:49,  3.44it/s]  0%|          | 103/166530 [00:25<13:20:25,  3.47it/s]  0%|          | 104/166530 [00:25<14:04:29,  3.28it/s]  0%|          | 105/166530 [00:25<13:50:42,  3.34it/s]  0%|          | 106/166530 [00:26<14:03:17,  3.29it/s]  0%|          | 107/166530 [00:26<13:00:55,  3.55it/s]  0%|          | 108/166530 [00:26<12:23:51,  3.73it/s]  0%|          | 109/166530 [00:26<11:49:01,  3.91it/s]  0%|          | 110/166530 [00:27<11:59:44,  3.85it/s]                                                         0%|          | 110/166530 [00:27<11:59:44,  3.85it/s]  0%|          | 111/166530 [00:27<12:03:07,  3.84it/s]  0%|          | 112/166530 [00:27<12:46:47,  3.62it/s]  0%|          | 113/166530 [00:27<12:42:31,  3.64it/s]  0%|          | 114/166530 [00:28<12:26:41,  3.71it/s]  0%|          | 115/166530 [00:28<13:12:34,  3.50it/s]  0%|          | 116/166530 [00:28<12:09:25,  3.80it/s]  0%|          | 117/166530 [00:28<11:22:54,  4.06it/s]  0%|          | 118/166530 [00:29<10:37:01,  4.35it/s]  0%|          | 119/166530 [00:29<10:07:58,  4.56it/s]  0%|          | 120/166530 [00:29<9:52:43,  4.68it/s]                                                         0%|          | 120/166530 [00:29<9:52:43,  4.68it/s]  0%|          | 121/166530 [00:29<9:29:34,  4.87it/s]  0%|          | 122/166530 [00:29<9:32:51,  4.84it/s]  0%|          | 123/166530 [00:30<9:25:40,  4.90it/s]  0%|          | 124/166530 [00:30<9:20:22,  4.95it/s]  0%|          | 125/166530 [00:30<9:35:29,  4.82it/s]  0%|          | 126/166530 [00:30<9:29:01,  4.87it/s]  0%|          | 127/166530 [00:30<9:30:44,  4.86it/s]  0%|          | 128/166530 [00:31<9:28:09,  4.88it/s]  0%|          | 129/166530 [00:31<9:21:29,  4.94it/s]  0%|          | 130/166530 [00:31<9:24:31,  4.91it/s]                                                        0%|          | 130/166530 [00:31<9:24:31,  4.91it/s]  0%|          | 131/166530 [00:31<9:27:56,  4.88it/s]  0%|          | 132/166530 [00:31<9:30:04,  4.86it/s]  0%|          | 133/166530 [00:32<9:27:48,  4.88it/s]  0%|          | 134/166530 [00:32<9:26:22,  4.90it/s]  0%|          | 135/166530 [00:32<9:24:47,  4.91it/s]  0%|          | 136/166530 [00:32<9:12:02,  5.02it/s]  0%|          | 137/166530 [00:32<9:14:50,  5.00it/s]  0%|          | 138/166530 [00:33<9:36:03,  4.81it/s]  0%|          | 139/166530 [00:33<9:30:57,  4.86it/s]  0%|          | 140/166530 [00:33<9:28:01,  4.88it/s]                                                        0%|          | 140/166530 [00:33<9:28:01,  4.88it/s]  0%|          | 141/166530 [00:33<9:25:12,  4.91it/s]  0%|          | 142/166530 [00:34<9:31:32,  4.85it/s]  0%|          | 143/166530 [00:34<9:26:25,  4.90it/s]  0%|          | 144/166530 [00:34<9:24:12,  4.92it/s]  0%|          | 145/166530 [00:34<9:52:23,  4.68it/s]  0%|          | 146/166530 [00:34<9:39:08,  4.79it/s]  0%|          | 147/166530 [00:35<9:29:25,  4.87it/s]  0%|          | 148/166530 [00:35<9:23:18,  4.92it/s]  0%|          | 149/166530 [00:35<9:17:28,  4.97it/s]  0%|          | 150/166530 [00:35<9:13:13,  5.01it/s]                                                        0%|          | 150/166530 [00:35<9:13:13,  5.01it/s]  0%|          | 151/166530 [00:35<9:11:27,  5.03it/s]  0%|          | 152/166530 [00:36<9:15:27,  4.99it/s]  0%|          | 153/166530 [00:36<9:45:11,  4.74it/s]  0%|          | 154/166530 [00:36<10:16:37,  4.50it/s]  0%|          | 155/166530 [00:36<10:16:38,  4.50it/s]  0%|          | 156/166530 [00:36<10:01:23,  4.61it/s]  0%|          | 157/166530 [00:37<10:11:38,  4.53it/s]  0%|          | 158/166530 [00:37<10:40:21,  4.33it/s]  0%|          | 159/166530 [00:37<11:01:56,  4.19it/s]  0%|          | 160/166530 [00:37<10:22:46,  4.45it/s]                                                         0%|          | 160/166530 [00:37<10:22:46,  4.45it/s]  0%|          | 161/166530 [00:38<11:26:08,  4.04it/s]  0%|          | 162/166530 [00:38<10:54:32,  4.24it/s]  0%|          | 163/166530 [00:38<10:28:48,  4.41it/s]  0%|          | 164/166530 [00:38<10:00:21,  4.62it/s]  0%|          | 165/166530 [00:38<9:40:06,  4.78it/s]   0%|          | 166/166530 [00:39<9:25:44,  4.90it/s]  0%|          | 167/166530 [00:39<9:32:22,  4.84it/s]  0%|          | 168/166530 [00:39<9:25:15,  4.91it/s]  0%|          | 169/166530 [00:39<11:34:55,  3.99it/s]  0%|          | 170/166530 [00:40<11:07:19,  4.15it/s]                                                         0%|          | 170/166530 [00:40<11:07:19,  4.15it/s]slurmstepd-idc-beta-batch-pvc-node-12: error: *** JOB 19462 ON idc-beta-batch-pvc-node-12 CANCELLED AT 2023-09-19T01:03:37 ***
